---
title: "LPL lab mussel salinity trials"
output: html_document
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---


# TODOs

* Write code to import all gape files from a particular mussel (SN###) and
  concatenate them into a single file

* Process those concatenated data to convert raw gape values into estimates of
gape opening %

* Write code to import and concatenate heart data files for each mussel

* Write code to process raw heart data into beats per minute estimates, with
manual quality assurance as needed

* Other stuff


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
showcode = FALSE # Set TRUE to print out all code in the output document
evalAll = FALSE # Set TRUE to run all chunks including the original data 
# ingestion, which can take 30 minutes+. 
options(digits.secs = 3) # Necessary to deal with sub-second time stamps in 
# heart data
```

# Libraries
```{r loadLibraries, echo=TRUE, message=FALSE}
library(tidyverse)
library(readxl)

# This will also use the libraries signal, pracma, and forecast, but will
# only load them as needed to avoid masking functions from other libraries

#library(scales)
#library(readr) 
#library(dplyr)
#library(Tides)
#library(janitor)
#library(zoo)
#library(esquisse)
#library(timetk)
#library(ggpattern)
```


# Setup

Define paths to files and create functions to handle data

```{r fileLocations, echo=FALSE}
# Path from the R code directory to the data directory
localpath = '../../data/Lab_mussel_salinity_trials/LPL_Mytilus_lab_salinity_trials_Summer2023/'  # Luke's path from the R cod dir to the data dir

# Path to the folder with the raw data subfolders
trialpath = 'Raw_data_files_Summer2023/Trial_data_Aug-Sep_2023/'

# Path to directory to hold concatenated gape files
outputgapedir = 'Concatenated_gape_files/'

# Path to directory to hold concatenated heart files
outputheartdir = 'Concatenated_heart_files/'

# Path to directory to hold processed estimated heart rates
outputheartratedir = 'Heart_rate_estimates/'

# Path to the metadata file
metafilename = 'Mussel_metadata_calibs_Summer2023.xlsx'

# File containing the trial salinity/temperature/dissolved oxygen data
trialfilename = 'Mussel_salinity_temp_data_Summer2023.xlsx'

```


```{r gapeFunctions,echo=FALSE}

#' HalltoMM function - use this to convert raw Hall effect values to estimates
#' of gape opening distance (mm), using calibration data in the CalibFits data
#' frame
#' 
#' @param GapeData A data frame containing a column of raw Hall values and a 
#' column called 'SN' containing the data logger serial number
#' @param CalibFits A data frame containing a column called 'SerialNumber' that
#' is used to match up with the SN value in the GapeData data frame, along with
#' a column 'Intercept' and a column 'Slope' containing coefficients from a
#' linear regression of Hall readings vs. known gape distance
#' 
#' @return Adds a column called gape.mm into the GapeData data frame with the 
#' estimated gape distance in mm. Values of gape that calculate out to less
#' than zero are truncated to zero millimeters.

HalltoMM = function(GapeData, CalibFits) {
	# Find the data in CalibFits that match the serialnumber, and estimate the
	# gape opening in millimeters using the stored intercept and slope values
	# in CalibFits.
	GapeData$gape.mm = (GapeData$Hall * 
				CalibFits$Slope[which(CalibFits$SerialNumber == GapeData$SN[1])]) + 
			CalibFits$Intercept[which(CalibFits$SerialNumber == GapeData$SN[1])]
	# Truncate any values below zero
	GapeData$gape.mm[which(GapeData$gape.mm < 0)] = 0
	 # Return the input data frame with its new data column
	GapeData
}

#####################################################################
#' calcPercentGape function
#' 
#' This function assumes that the input values are already converted from raw
#' Hall sensor values to estimates of gape distance in millimeters. It returns
#' a vector of gape values expressed as a percentage of the fully open distance
#' in the range 0-100%.
#' 
#' @param gape.mm A vector of shell gape values measured in millimeters
#' 
#' @return A vector of gape values converted to a percentage opening (0-100%) 

calcPercentGape <- function(gape.mm){
	# Convert to percentage by dividing all distances by the maximum value
	outputData = (gape.mm / max(gape.mm,na.rm=TRUE)) * 100
	
	# Round the values off to a reasonable precision
	outputData = round(outputData,1)
	
	# Return the vector of percentage values (0-100%)
	outputData
}



# A set of functions originally taken from MusselTracker_data_proc4.R to 
# handle the gape data

################################################################################
### gapBounds function
# A function to determine where there are gaps of missing data, so that we can 
# apply a filter to the good chunks of data. Most filters fail if there are NAs
# present, and our remaining gaps in the data are big enough that they probably
# shouldn't be smoothed-over anyhow. 
#' A function to find the start and end indices of numeric data in a vector. 
#' Useful for dealing with timeseries that have gaps, and operations that need
#' to operate the separate chunks of contiguous numeric values
#' @param value A vector of numeric values, with NAs 
#' @return A data frame with 2 columns, Start and End, which contain the row
#' indices for the first good and last good values in each contiguous run of
#' numeric values. 	
gapBounds <- function (values){
	# This function returns a data frame of 2 columns, start + end, that give
	# the row indices of the start and end of each run of good data in the 
	# input vector. It should handle data streams that start with NAs and
	# should handle streams that end with NAs. 
	
	# Use run length encoding function to find NA gaps
	gaps = rle(is.na(values))
	# If the first entry in gaps is TRUE, this indicates that the data started
	# with NA values.
	if (gaps$values[1] == TRUE) {
		startNA = TRUE	# Data started with NAs
	} else {	
		startNA = FALSE	 # Data started with real values
	}
	# If the last entry in gaps is TRUE, the data end on NAs
	if (gaps$values[length(gaps$values)] == TRUE){
		endNA = TRUE	# Data ends with NAs
	} else {
		endNA = FALSE	# Data ends with real values
	}
	
	# The number of gaps with value == TRUE is the number of good runs of data
	# and gaps with value == FALSE are the starts of runs of NAs. This will
	# get the number of FALSE (negated to TRUE) values in gaps. A dataset
	# ending in good data will have a last entry in gaps of FALSE. 
	numgaps = sum(!gaps$values) 
	
	# Create the output data frame
	results = data.frame(Start = integer(numgaps), End = integer(numgaps))
	
	if (!startNA) {
		# If startNA == FALSE (started on good data, the first entry should 
		# always be 1 (1st row) 
		# If there are no gaps, the
		# contents of gaps$lengths will just be the length of the values vector
		results$Start[1] = 1
		results$End[1] = gaps$lengths[1]
	} else if (startNA){
		# However, if the dataset starts with NAs, the first entry in gaps will
		# be the index of the first good data, while the 2nd entry will be the 
		# start of the next stretch of NAs
		results$Start[1] = gaps$lengths[1]+1
		results$End[1] = sum(gaps$lengths[1:2])
	}
	
	# If there is more than 1 entry in gaps$lengths, process the other gaps
	j = 2; # counter
	if (numgaps > 1){
		if (!startNA & endNA){
			# If the dataset ends on NAs (TRUE last), truncate gapind by 1
			gapind = seq(2,length(gaps$lengths)-1, by = 2)
		} else if (!startNA & !endNA) {
			# If the dataset ends on good values (FALSE last)
			gapind = seq(2,length(gaps$lengths), by = 2)
		} else if (startNA & endNA) {
			# If dataset starts on NAs (TRUE 1st) and ends on NAs (TRUE last)
			gapind = seq(3, length(gaps$lengths)-1, by = 2)
		} else if (startNA & !endNA) {
			# If dataset starts on NAs (TRUE 1st) and ends on good data 
			# (FALSE last)
			gapind = seq(3, length(gaps$lengths), by = 2)
		}	
		# Step through the rest of the gaps object to find the start and end
		# points of each stretch of good data. 
		for (i in gapind){
			nextstart = sum(gaps$lengths[1:i]) + 1
			nextend = sum(gaps$lengths[1:(i+1)])
			results$Start[j] = nextstart
			results$End[j] = nextend
			j = j + 1
		}
	} # end of if (numgaps > 1)
	results	# return the results dataframe, 2 columns Start and End
}


```

# Import the data

## Import metadata

```{r importMetadata}
# readxl package is part of the tidyverse packages

# Import the calibration sheet
gapeCalibs = readxl::read_excel(paste0(localpath, metafilename), 
		sheet = 'GapeCalibrations')
# Convert MusselID to factor, and remove the underscore in the new column name
gapeCalibs$MusselID = factor(gapeCalibs$Mussel_ID)
# Convert from tibble garbage to a regular data frame for regular people
gapeCalibs = data.frame(gapeCalibs)

# Import the mussel metadata sheet
musselMetaData = readxl::read_excel(paste0(localpath, metafilename), 
		sheet = 'MusselMetadata')
musselMetaData$MusselID = factor(musselMetaData$MusselID)

# Import the sheet with information on start/stop times for trials
trialMetaData = readxl::read_excel(paste0(localpath, metafilename), 
		sheet = 'TrialMetadata')
trialMetaData$MusselID = factor(trialMetaData$MusselID)
trialMetaData$SerialNumber = factor(trialMetaData$SerialNumber)
trialMetaData$SalinityTreatment = factor(trialMetaData$SalinityTreatment)
# The metadata times were recorded in local time (Pacific Daylight Time), but 
# the read_excel function assumes they're UTC time zone. Use this to swap them
# to Pacific Daylight Time zone without actually changing the time value
trialMetaData$AcclimationStartTimePDT = 
		lubridate::force_tz(trialMetaData$AcclimationStartTime, tzone = 'PST8PDT')
trialMetaData$TrialStartTimePDT = 
		lubridate::force_tz(trialMetaData$TrialStartTime, tzone = 'PST8PDT')
trialMetaData$TrialStopTimePDT = 
		lubridate::force_tz(trialMetaData$TrialStopTime, tzone = 'PST8PDT')


```

### Process the calibration data

Mussel gape was calibrated using rods of known diameter stuck into the gaping
shells at the location of the hall effect sensor/magnet combination, and then
recording the raw Hall effect reading while the mussel was clamped down on the
rod. 

```{r parseGapeCalibData, echo = showcode}
# Process the gape calibration data for each mussel. They are in long format
# so you need to find all the rows with data for a given mussel, fit a 
# regression to gauge diameter vs Hall reading, and retain the slope/intercept
# information for that mussel

# Get a list of mussel IDs
ids = unique(gapeCalibs$MusselID)


for (i in 1:length(ids)){
	# Subset out all rows for this ID
	temp = gapeCalibs[which(gapeCalibs$MusselID == ids[i]),]
	
	# If there are more than 2 gape measurements, attempt to fit the regression
	# and otherwise skip this mussel (and perhaps print a warning)
	if (nrow(temp) > 2){
		# Fit a regression line through these data points
		mymod = lm(GaugeDiameter.mm ~ HallReading, data = temp)
		if (i == 1){
			# Save the intercept and slope coefficients in a list
			CalibFits = data.frame(MusselID = as.character(ids[i]),
					Intercept = mymod$coefficients["(Intercept)"], 
					Slope = mymod$coefficients[2])	
		} else if (i > 1) {
			CalibFits = rbind(CalibFits, 
					data.frame(MusselID = as.character(ids[i]),
							Intercept = mymod$coefficients["(Intercept)"], 
							Slope = mymod$coefficients[2]))
		}
	} else {
		warning('Not enough calibration data to process ', ids[i])
	}
}

# Add the datalogger serial number to each mussel (this will need future work
# if a mussel switches between datalogger serial numbers during use)
CalibFits$SerialNumber = NA
for (i in 1:nrow(CalibFits)){
	whichSN = as.character(trialMetaData$SerialNumber[which(as.character(trialMetaData$MusselID) == 
							as.character(CalibFits$MusselID[i]))])
	if (length(whichSN) > 0){
		CalibFits$SerialNumber[i] = whichSN
	}
}

# With the output CalibFits, you should be able to take any raw Hall reading
# for a mussel (matching MusselID) and estimate a gape opening using the
# regression slope and intercept stored in CalibFits

```



## Import and concatenate gape files

The individual daily valve gape files need to be imported for each 
datalogger/mussel and concatenated into a single long file for each animal. The
resulting files are written as CSV files that can be reopened later in the 
script for further processing. The datalogger clocks should all have been set
to UTC time zone, so this script assumes that when converting dates and times 
in the data files.

```{r concatGape, echo=showcode, eval = evalAll}
# In practice you should only need to run this chunk 1 time to generate the 
# concatenated gape data files for each datalogger (serial number). If new data
# are later collected for that same datalogger/mussel, you'd need to run this
# again to generate the new, longer concatenated file. Otherwise, you should
# primarily be working from the concatenated data files in later chunks

# Get a list of datalogger serial numbers to process
SNnames = dir(path=paste0(localpath,trialpath),recursive=FALSE)
# Should return folder names like "SN103", "SN173"

for (sn in SNnames){
	# sn will hold the text representation of one of the folders (i.e. 'SN103')
	# Get a list of the GAPE files in the appropriate data folder
	fnames = dir(path=paste0(localpath,trialpath,sn), pattern="*GAPE.csv",
			full.names=TRUE)
	
	tempData = ConcatGapeFiles(filenames=fnames,myTimeZone = 'UTC',verbose=TRUE)
	# Take the data in tempData and put it in a data frame named based on 
	# the serial number being processed
#	assign(sn,tempData)
	# Write the concatenated data to a csv file
	write.csv(tempData, file = paste0(localpath,outputgapedir,sn,'_gape.csv'),
			row.names=FALSE)
}

# Output times are still in UTC time zone


```

## Import and concatenate heart data

The individual daily heart data files for each datalogger/mussel need to be 
imported, parsed, and concatenated into one long output file per datalogger.
The script assumes that in this case the sampling rate for heart data was 8
samples per second and that a set of 400 samples was collected during each
sampling interval. 400 samples at 8Hz = 50 seconds of samples. In this lab trial
loggers were set to collect a new set of samples every 2 minutes (every even
minute). The concatenated data are written as CSV files that can be reopened
later in the script for further processing rather than having to do this long
ingestion process every time. The datalogger clocks should all have been set to
UTC time zone, so this script assumes that when converting dates and times in
the data files.

```{r concatHeart,echo=showcode, eval = evalAll}
# In practice you should only need to run this chunk 1 time to generate the 
# concatenated gape data files for each datalogger (serial number). If new data
# are later collected for that same datalogger/mussel, you'd need to run this
# again to generate the new, longer concatenated file. Otherwise, you should
# primarily be working from the concatenated data files in later chunks

# This chunk will take 10's of minutes to run if you have lots of files
# to process (much longer than the gape processing)

#########################################
# Get a list of datalogger serial numbers to process
SNnames = dir(path=paste0(localpath,trialpath),recursive=FALSE)
# Should return folder names like "SN103", "SN173"

for (sn in SNnames){
	# sn will hold the text representation of one of the folders (i.e. 'SN103')
	# Get a list of the IR (heart sensor) files in the appropriate data folder
	hnames = dir(path=paste0(localpath,trialpath,sn), pattern="*IR.csv",
			full.names=TRUE)
	
	tempData = ConcatHeartFiles(filenames = hnames, myTimeZone = 'UTC', 
			verbose = TRUE, sampleLength = 400)
	# Take the data in tempData and put it in a data frame named based on 
	# the serial number being processed
#	assign(sn,tempData)
	
	# Write the concatenated data to a csv file
	write.csv(tempData, file = paste0(localpath,outputheartdir,sn,'_heart.csv'),
			row.names=FALSE)
}

# Output times are still in UTC time zone

```


## Import salinity/temperature/DO data from trials

TODO: Add code to import associated water quality parameters during the trials

# Process gape data

The concatenated gape files will contain gape data from before, during, and
after the trials, so we'll want to subset out the relevant chunks of time
for the mussels involved in a particular trial. The time data should be stored
in the `trialMetaData` data frame. 





```{r loadConcatGapeDataTrial1, echo=showcode}
# Reimport the concatenated gape data for relevant mussels

# Get the list of MusselID's / SerialNumbers for each mussel involved in Trial1

myTrial = 1

snsControl = as.character(
		trialMetaData$SerialNumber[which(
						trialMetaData$SalinityTreatment == 'control' & 
						trialMetaData$TrialNumber == myTrial)])

snsLowSal = as.character(
		trialMetaData$SerialNumber[which(
						trialMetaData$SalinityTreatment == '3psu' & 
								trialMetaData$TrialNumber == myTrial)])

# Be aware that time stamps in the concatenated gape files are still in UTC
# time zone when imported here

for (i in 1:length(snsControl)){
	temp = read.csv(file = paste0(localpath,outputgapedir,
					snsControl[i],'_gape.csv'))
	# Get start and stop time from trialMetaData 
	startTime = trialMetaData$TrialStartTimePDT[which(trialMetaData$SerialNumber ==
							snsControl[i])]
	stopTime =  trialMetaData$TrialStopTimePDT[which(
					trialMetaData$SerialNumber == snsControl[i])]

	# Convert DateTime into POSIXct format and shift to PDT time zone
	temp$DateTime = as.POSIXct(temp$DateTime, tz = 'UTC')
	attr(temp$DateTime, 'tzone') = 'PST8PDT'  # shift to Pacific Time
	# Trim the start and stop times
	temp = temp[which(temp$DateTime >= startTime),]
	temp = temp[which(temp$DateTime <= stopTime),]
	
	assign(paste0(snsControl[i],'gape'), value = temp)
}


for (i in 1:length(snsLowSal)){
	temp = read.csv(file = paste0(localpath,outputgapedir,
					snsLowSal[i],'_gape.csv'))
	# Get start and stop time from trialMetaData 
	startTime = trialMetaData$TrialStartTimePDT[which(trialMetaData$SerialNumber ==
							snsLowSal[i])]
	stopTime =  trialMetaData$TrialStopTimePDT[which(
					trialMetaData$SerialNumber == snsLowSal[i])]
	
	# Convert DateTime into POSIXct format and shift to PDT time zone
	temp$DateTime = as.POSIXct(temp$DateTime, tz = 'UTC')
	attr(temp$DateTime, 'tzone') = 'PST8PDT'
	# Trim the start and stop times
	temp = temp[which(temp$DateTime >= startTime),]
	temp = temp[which(temp$DateTime <= stopTime),]
	
	assign(paste0(snsLowSal[i],'gape'), value = temp)
}

# After these imports, the DateTime column in each of the gape data frames is
# now in PST8PDT time zone instead of UTC. The data are also trimmed to the 
# start and stop times of the trial


quickplot = function(GapeData){
	plot(GapeData$DateTime, y = GapeData$Hall, type = 'l')
}

# Controls, Trial 1
# quickplot(SN173gape)  # data look okay
# quickplot(SN196gape)  # data look okay
# quickplot(SN224gape)  # data look okay

# Low salinity Trial 1
#quickplot(SN189gape)  # data look okay - low for middle of trial
#quickplot(SN218gape)  # data look okay - low for middle of trial
# quickplot(SN103gape) # no data past 8/24/2023, don't use
#quickplot(SN179gape)  # data look okay - low for middle of trial

# Apply the calibration data to estimate gape opening in mm for each mussel

SN173gape = HalltoMM(SN173gape,CalibFits)
SN196gape = HalltoMM(SN196gape, CalibFits)
SN224gape = HalltoMM(SN224gape, CalibFits)
SN189gape = HalltoMM(SN189gape, CalibFits)
SN218gape = HalltoMM(SN218gape, CalibFits)
SN179gape = HalltoMM(SN179gape, CalibFits)

SN173gape$gape.pc = calcPercentGape(SN173gape$gape.mm)
SN196gape$gape.pc = calcPercentGape(SN196gape$gape.mm)
SN224gape$gape.pc = calcPercentGape(SN224gape$gape.mm)
SN189gape$gape.pc = calcPercentGape(SN189gape$gape.mm)
SN218gape$gape.pc = calcPercentGape(SN218gape$gape.mm)
SN179gape$gape.pc = calcPercentGape(SN179gape$gape.mm)

# Time values in these data frames are in local PST8PDT time zone


```
From the 1st trial, there appears to be good gape data from control mussels
SN173, SN196, and SN224. For the low salinity mussels, SN189, SN218, and SN179
appear to be usable. 



```{r loadConcatGapeDataTrial2, echo=showcode}
# Process the 2nd trial's gape data
myTrial = 2

snsControl = as.character(
		trialMetaData$SerialNumber[which(
						trialMetaData$SalinityTreatment == 'control' & 
								trialMetaData$TrialNumber == myTrial)])

snsLowSal = as.character(
		trialMetaData$SerialNumber[which(
						trialMetaData$SalinityTreatment == '3psu' & 
								trialMetaData$TrialNumber == myTrial)])


for (i in 1:length(snsControl)){
	temp = read.csv(file = paste0(localpath,outputgapedir,
					snsControl[i],'_gape.csv'))
	# Get start and stop time from trialMetaData 
	startTime = trialMetaData$TrialStartTimePDT[which(trialMetaData$SerialNumber ==
							snsControl[i])]
	stopTime =  trialMetaData$TrialStopTimePDT[which(
					trialMetaData$SerialNumber == snsControl[i])]
	
	# Convert DateTime into POSIXct format and shift to PDT time zone
	temp$DateTime = as.POSIXct(temp$DateTime, tz = 'UTC')
	attr(temp$DateTime, 'tzone') = 'PST8PDT'
	# Trim the start and stop times
	temp = temp[which(temp$DateTime >= startTime),]
	temp = temp[which(temp$DateTime <= stopTime),]
	
	assign(paste0(snsControl[i],'gape'), value = temp)
}


for (i in 1:length(snsLowSal)){
	temp = read.csv(file = paste0(localpath,outputgapedir,
					snsLowSal[i],'_gape.csv'))
	# Get start and stop time from trialMetaData 
	startTime = trialMetaData$TrialStartTimePDT[which(trialMetaData$SerialNumber ==
							snsLowSal[i])]
	stopTime =  trialMetaData$TrialStopTimePDT[which(
					trialMetaData$SerialNumber == snsLowSal[i])]
	
	# Convert DateTime into POSIXct format and shift to PDT time zone
	temp$DateTime = as.POSIXct(temp$DateTime, tz = 'UTC')
	attr(temp$DateTime, 'tzone') = 'PST8PDT'
	# Trim the start and stop times
	temp = temp[which(temp$DateTime >= startTime),]
	temp = temp[which(temp$DateTime <= stopTime),]
	
	assign(paste0(snsLowSal[i],'gape'), value = temp)
}

# DateTime values are shifted to local time PST8PDT

# Trial 2 control
#quickplot(SN214gape) # data look okay
#quickplot(SN242gape) # stayed closed entire trial
#quickplot(SN217gape) # Not collecting data during trial
#quickplot(SN151gape) # no data, malfunctioning sensor reading 1023 only

# Trial2 # low salinity treatment
#quickplot(SN243gape)  # fully closed entire trial until very end - probably okay
#quickplot(SN198gape)  # briefly open at start and very end of trial - probaly okay
#quickplot(SN245gape)  # malfunctioning sensor reads 1023 only
#quickplot(SN226gape)  # malfunctioning sensor during trial drifts > 500



SN214gape = HalltoMM(SN214gape, CalibFits)
SN242gape = HalltoMM(SN242gape, CalibFits)
SN243gape = HalltoMM(SN243gape, CalibFits)
#SN198gape = HalltoMM(SN198gape, CalibFits) # need calibration data

SN214gape$gape.pc = calcPercentGape(SN214gape$gape.mm)
SN242gape$gape.pc = calcPercentGape(SN242gape$gape.mm)
SN243gape$gape.pc = calcPercentGape(SN243gape$gape.mm)
#SN198gape$gape.pc = calcPercentGape(SN198gape$gape.mm) # need calibration data

# DateTime values are shifted to local time PST8PDT

```

For the 2nd trial, it appears that there are only usable gape data for two of
the control mussels (SN214 and SN242, but 242 was closed the entire time), and
gape data from low-salinity mussels SN243 and SN198 (but SN198 lacks 
calibration data currently). 


# Process heart data

#getBPM function
```{r getBPMfunction,echo=showcode}
# A function to convert raw heartbeat timeseries into estimates of heart rate
# (beats per minute). This function uses the filtfilt routine from the signal
# library and a butterworth filter. If you specify a particular frequency 
# for the lowband (and optionally highband), it will effectively filter
# at half that frequency. For example, if you specify a lowband value of 
# 1/10 (0.1 = 10 Hz), you get a lowpass filter that cuts off signals above around 5Hz.
# If you specify 1/20 (0.05 = 5Hz), the lowpass filter filters anything above
# about 2.5Hz.

#' @param x A data frame with columns DateTime and IR for sensor data
#' @param t1 The starting index of data to be subset from x
#' @param sensor Column name in x declaring the sensor that you want to process
#' @param chunklength Number of samples to process, default = 400 is 50 seconds at 8Hz for BivalveBit loggers
#' @param lowband Frequency for the lower frequency of the bandpass filter, consider values from 1/20 to 1/300
#' @param highband Frequency for the upper frequency of the bandpass filter, consider values from 1/2 to 1/20. Set to NULL to 
#' implement a lowpass filter only
#' @param amplitudeThreshold Minimum peak to peak amplitude of the *filtered* heart signal
#' @param maxBPM Maximum beats per minute estimate, higher values will be flagged in the output
#' @param Fs Sampling interval in seconds (default 0.125 = 8Hz for BivalveBit loggers)
#' @param BPMqualityThreshold A numeric value indicating how close two heart rate estimates must be to be considered in agreement 
#' @param plot Logical value to show a plot or not. Default is FALSE.
#' @param myYlims Set of fixed y-axis limits for plots
#' 
#' 
#' @return A list containing 3 estimates of the beats per minute, along with
#' values indicating how close the 3 estimates agree and whether the signal may
#' be questionable.

getBPM = function(x,t1 = 1, sensor = 'IR',chunklength=400, 
		lowband = 1/100, 
		highband = 1/10, 
		amplitudeThreshold = 60,
		maxBPM = 80,
		Fs = 0.125,
		BPMqualityThreshold = 4,
		plot = FALSE,
		myYlims = NULL)
{
	temp = x[t1:(t1+chunklength-1),]  #Grab the chunk of data
	
	# 
	# Grab a chunk of data that's twice as long as we need
#	temp = x[t1:(t1+(chunklength*2)),]
	# Calculate time difference between each sample (milliseconds), should be 100
#	diffs = diff(temp$startMillis)
	# Identify any gaps in the data where the interval was greater than the sampling interval in milliseconds
#	missedReads = which(diffs > (1000 * Fs))
	
# Check if there are any sampling gaps (missedReads)
#	if (length(missedReads) > 0){
#		# Add on the final row as well
#		missedReads = c(missedReads, (nrow(temp)+1) )
#		# Calculate gap length between any missed reads (and the final read)
#		testgaps = diff(missedReads) 
#		if (missedReads[1] > chunklength){
#			# In this case, just grab the first 300 readings
#			temp = temp[1:chunklength,]
#		} else if (length(which(testgaps>=chunklength)) > 0){
#			# check if any of the testgaps values are >=300 (chunklength)
#			# Get the index in temp that is at the start of the long run
#			tempindx = missedReads[which(testgaps >= chunklength)]+1
#			# If a gap is > 300, grab the sample in that gap
#			temp = temp[tempindx:(tempindx+chunklength-1),]
#		} else if (length( which(testgaps>=chunklength) ) == 0) {
#			# In this case there may be multiple gaps in the time chunk, 
#			# so that there are no good contiguous chunks of 300 readings
#			# Return a data frame with NAs
#			temp[,sensor] = NA 
#			
##			tempindx = missedReads[length(missedReads)]+1
##			temp = temp[tempindx:(tempindx+chunklength-1),]
#			# It's also possible to end up here if you grabbed a chunk that
#			# ran off the end of the 600 samples and returned some NAs. That
#			# will be handled below
#			
#		} 
#	} else if (length(missedReads) == 0) {
#		# Subset down to exactly the chunk length (10Hz sample * 30 secs = 300 samples)
#		temp = temp[1:chunklength,]
#	}
	

	# Add milliseconds onto the timestamps
	temp$DateTimeMS = temp$DateTime
	# create a new set of timestamps that have millisecond values attached
#	myvec=vector(mode="numeric", length=chunklength-1)
#	myvec[1:length(myvec)]=Fs
#	temp$DateTimeMS[2:nrow(temp)] = temp$DateTimeMS[1] + cumsum(myvec)

	
	# Test if there are any NAs in the heart rate values, if there are not
	# then proceed with the filtering and heart rate determination
	if ( length( which( is.na(temp[,sensor]) ) ) == 0){
		
		# Detrend the heartrate readings
		detrendedIR = pracma::detrend(temp[,sensor])	
		# Next look at the detrended values and look for spurious low values
		# For instance, a good heart signal might oscillate between -200 & +300
		# in the detrended data, and a spurious value might suddenly drop to 
		# -6000. 
		# Calculate the standard deviation of the data set, and then find values
		# that are more than 3 SD away from the mean (which should be ~zero in the
		# detrended data)
#		spuriousVals = which(abs(detrendedIR) > (3*sd(detrendedIR)) )

#		if ( length(spuriousVals) > 0) {
#			# Convert to NAs 
#			temp[spuriousVals,sensor] = NA
#			
#			for (i in 1:length(spuriousVals)){
#				indx = spuriousVals[i]  # Get the row index for this spurious value
#				if (indx == 1) {
#					# If the first value is spurious, replace it with a copy 
#					# of the next value
#					temp[indx,sensor] = temp[indx+1,sensor]
#				} else {
#				# Replace the spurious value with the average of the values immediately
#				# before and after the spurious value
##				temp[indx,sensor] = mean(c(temp[indx-1,sensor],temp[indx+1,sensor]))
#				temp[indx,sensor] = mean(temp[(indx-2):(indx+2),sensor], na.rm=TRUE)
#				}
#			}
#		}
#		rm(spuriousVals)

		
		# With the spurious values replaced by interpolated values, re-run the 
		# detrending routine
#		detrendedIR = pracma::detrend(temp[,sensor])
	} else {
		# If there were NAs, just define detrendedIR as NA so that later 
		# operations skip over this chunk of data
		detrendedIR = NA
	}

	

	if (length(which(is.na(detrendedIR))) == 0 & (var(detrendedIR) > 0) ) {

	# Define a butterworth filter
	# Consider using the bandpass filter rather than just a lowpass filter
	# because of the tendency for the IR heartrate signal to drift up and 
	# down in relation to ambient light, which tends to induce low-frequency
	# shifts that then fool the spectral analysis routines when trying to 
	# identify the dominant frequency
		if (!is.null(highband)) {
			bf = signal::butter(3,W = c(lowband, highband), type = 'pass')	# bandpass filter
		} else if (is.null(highband)){
			bf = butter(3,W = lowband, type = 'low')  #  lowpass filter			
		}

		# Apply the filter to the detrended data chunk
		y = filtfilt(bf, x = detrendedIR)
		# Calculate the spectrum of the filtered data
		myfft2 = spectrum(y, plot = FALSE)
		# Take the peak frequency from the spectrum, divide by sampling
		# rate to convert to cycles per second
		mypeakfreq = myfft2$freq[which.max(myfft2$spec)] / Fs
		# Multiply by 60 seconds to get cycles (beats) per minute
		BPMfft = 60 * mypeakfreq
		# Calculate amplitude of filtered signal to denote weak or noisy signals
		amp = range(y)[2] - range(y)[1]
		# Use function from package 'forecast', returns peak period (not freq)
		forecastPeriod = forecast::findfrequency(y)  
		forecastFreq = 1/forecastPeriod # convert period to frequency
		BPMforecast = forecastFreq * 60 * (1/Fs) # convert frequency to beats per 
		# minute, based on the fact that the sampling period is Fs, and there are
		# 60 seconds in a minute

		# Set a flag for cases where the forecast:findfrequency estimate is 
		# extremely large, which happens when it can't find a clear heart signal
		BPMforecastflag = ifelse(BPMforecast > maxBPM, 'FAIL','OK')
		# Set a flag for cases where the detrended/filtered signal has a very
		# small amplitude, signaling that there may be no good heartbeat signal 
		WeakSignalFlag = ifelse(amp < amplitudeThreshold, 'FAIL','OK')
		# Use pracma package to find peaks. Note that at slower heart rates
		# this function tends to find the sub-peaks (akin to a P or T peak in a
		# human ECG trace) rather than just the main peaks (R peaks on a human). 
		# This happens based on what the bandpass filter lets through. 
		pracPeaks = pracma::findpeaks(x = y, nups = 5, minpeakdistance = 10)
		# Figure out what we need to multiply the number of peaks by in order
		# to get from our less-than-a-minute sampling period to a full minute
		samplingseconds = chunklength * Fs
		minutemultiplier = 60 / samplingseconds  
		# If we sampled for 30 seconds, minutemultiplier should be 2
		# if we sampled for 50 seconds, minutemultiplier should be 1.2
		# Now convert the number of peaks to the number of beats per minute
		BPMprac = nrow(pracPeaks) * minutemultiplier
		
		# Calculate the difference between the estimated heart rates from the 
		# spectrum fft routine and the forecast::findfrequency routine. A 
		# small value indicates good agreement
		BPMagreeQuality.fft.forecast = ceiling(abs(BPMfft - BPMforecast))
		# Calculate difference between the forecast and pracma estimates
		BPMagreeQuality.prac.forecast = ceiling(abs(BPMprac - BPMforecast))
		# Calculate difference between fft and pracma estimates
		BPMagreeQuality.prac.fft = ceiling(abs(BPMprac - BPMfft))
		
		if (plot){
			## Plot the raw detrended signal
			if (!is.null(myYlims)){
				# Use the specified y limits
				plot(temp$DateTimeMS, detrendedIR, type = 'l', 
						main = '', xlab = 'Seconds', ylab = '',
						las = 1, ylim = myYlims)	
			} else if (is.null(myYlims)) {
				# No y limits specified, use plot defaults
				plot(temp$DateTimeMS, detrendedIR, type = 'l', 
						main = '', xlab = 'Seconds', ylab = '',
						las = 1)	
			}
			 
			points(temp$DateTimeMS, detrendedIR, col = 1, pch = 20, cex = 1)
			if (amp > amplitudeThreshold){
				lines(temp$DateTimeMS, y, col = 3, lwd = 2) # add the filtered signal as a green line
			} else if (amp <= amplitudeThreshold) {
				lines(temp$DateTimeMS, y, col = 2, lwd = 2) # add the filtered signal as a red line	
				warning('Weak signal')
			}
			points(temp$DateTimeMS[pracPeaks[,2]], y = pracPeaks[,1], col = 4, pch = 19)
			mtext(side = 2, text = 'Detrended IR signal', line = 2.5, cex = 1)
			mtext(side = 1, line = 3, text = paste(sensor, strftime(temp$DateTime[1], tz="PST8PDT")), cex = 0.8, adj = 1)
			mtext(side = 3, line = 3, text = paste0('pracma bpm: ', BPMprac))
			mtext(side = 3, line = 2, text = paste0('fft bpm: ', BPMfft))
			mtext(side = 3, line = 1, text = paste0('forecast filtered bpm: ', round(BPMforecast,1)))
			if (!is.null(highband)){
				mtext(side = 3, line = 0.01, 
						text = paste0('Bandpass - Lower pass: ', round(lowband,digits =3), ', upper pass: ', round(highband,digits = 3)),
						cex = 0.8)	
			} else if (is.null(highband)) {
				mtext(side = 3, line = 0.01, 
						text = paste0('Lowpass - Lower limit: ', round(lowband,digits =3)),cex = 0.8)	
			}
			
			
		}

		resultsList = list(DateTime = temp$DateTime[1],
				Sensor = sensor,
				BPMfft = round(BPMfft,1),
				BPMforecast = round(BPMforecast,1),
				BPMpeaks = BPMprac,
				BPMagreeQuality.fft.forecast = BPMagreeQuality.fft.forecast,
				BPMagreeQuality.prac.forecast = BPMagreeQuality.prac.forecast,
				BPMagreeQuality.prac.fft = BPMagreeQuality.prac.fft,
				BPMforecastflag = BPMforecastflag,
				WeakSignalFlag = WeakSignalFlag,
				FilteredAmplitude = round(amp,1),
				finalBPM = NA,
				finalfilter = 1/highband,
				QA = FALSE)
		# If there is reasonable agreement between the 3 beats per minute estimates,
		# record the mean of the 3 values as the finalBPM
		if (resultsList$BPMagreeQuality.fft.forecast <= BPMqualityThreshold & 
				resultsList$BPMagreeQuality.prac.forecast <= BPMqualityThreshold &
				resultsList$BPMagreeQuality.prac.fft <= BPMqualityThreshold & 
				resultsList$WeakSignalFlag == 'OK') {
			resultsList$finalBPM = round(mean(resultsList$BPMfft,
							resultsList$BPMforecast,
							resultsList$BPMpeaks), digits = 1)
		}
	} else if (length( which( is.na(temp[,sensor]) ) ) > 0 | (var(detrendedIR) == 0)) {
		# Handle the case where there are NAs in the data chunk that prevent
		# the filtering and fft routines, or the sensor was disconnected and
		# only returning zeros (leading to no variance)
		resultsList = list(DateTime = temp$DateTime[1],
				Sensor = sensor,
				BPMfft = NA,
				BPMforecast = NA,
				BPMpeaks = NA,
				BPMagreeQuality.fft.forecast = NA,
				BPMagreeQuality.prac.forecast = NA,
				BPMagreeQuality.prac.fft = NA,
				BPMforecastflag = 'FAIL',
				WeakSignalFlag = 'FAIL',
				FilteredAmplitude = NA,
				finalBPM = NA,
				finalfilter = NA,
				QA = FALSE
				)
	}
	

	return(resultsList)
}

```



Heart processing workflow:  
* Import a concatenated heart file from one of the 'good' mussels that also have
 usable gape data
* Truncate heart data to the time period of the trial (optional, to speed
  processing)
* Run through the automated heart rate algorithm
* Quality check the heart rate estimates and potentially visually inspect
* Generate time-stamped set of heart rate estimates for each mussel


```{r processHeartDataLoop, echo=showcode, eval=FALSE}
# one-time run of all heart files, will take a few hours
# Trial 1 Control mussels SN173, SN196, and SN224. 
# Trial 1 low salinity mussels SN189, SN218, and SN179

# If you just want to process one datalogger
mySNs = 'SN173' 

# If you want to process a list of serial numbers all at once
mySNs = c('SN173','SN196','SN224','SN189','SN218','SN179','SN214','SN242',
		'SN243','SN198')




for (sn in 1:length(mySNs)){
	
	# Load the concatenated heart file
	SensorData = read.csv(file= paste0(localpath,outputheartdir,
					mySNs[sn],'_heart.csv'), na.strings = 'NA')
	SensorData$DateTime <- as.POSIXct(SensorData$DateTime,
			format = '%Y-%m-%d %H:%M:%OS', tz='UTC')
	
	# Figure out where the start of each sampling bout should be. 
	# The sampling bouts were typically separated by 2 minutes, so we'll just
	# look for any time gap that's bigger than 60 seconds. 
	steps = diff(SensorData$DateTime)
	bigsteps = which(steps > 60)
	bigsteps = bigsteps+1
	bigsteps = c(1,bigsteps) # A set of row indices in SensorData for each new 
	# sample period
	
	
	# mylowband- filters low freq signals out, gets rid of slow signals
	mylowband = 1/100  # filtering - ran some at 1/100 as a starting point.
	# 1/100 filters out more low freq stuff.Can go as low as 1/20, try 1/50 next
	#myhighband-get rid of high freq noise so you just get normal peaks. 
	# lower (1/20-->1/15) low band if have low long dips but actually have 
	# quick peaks in there. Bc the smaller it gets, then only lets 
	# through small range of freq
	myhighband = 1/10  # filtering - changing this has the largest effect, start around 1/15
	#1/5 letting more high freq peaks through. may also catch every little peak.
	myThreshold = 4 # Minimum agreement between BPM estimates
	
	pb = txtProgressBar(min = 0, max = length(bigsteps), style = 3)
	
	for (i in 1:length(bigsteps)){
		setTxtProgressBar(pb,i)
		res1 = getBPM(SensorData, t1 = bigsteps[i], sensor = 'IR', chunklength = 400,
				lowband = mylowband, highband = myhighband, amplitudeThreshold = 20,
				BPMqualityThreshold = myThreshold,
				maxBPM = 50, Fs = 0.125, plot = FALSE)
		if (i == 1) {			
			SensorResult = as.data.frame(res1)
			SensorResult[2:length(bigsteps),] = NA
		} else {
			SensorResult[i,] = as.data.frame(res1)
		}
	}
	close(pb)

	rm(res1, SensorData)
	
	# Save an output file for faster loading in next chunks
	write.csv(SensorResult,
			file = paste0(localpath,outputheartratedir,mySNs[sn],'_bpm.csv'),  
			row.names=FALSE)

} # end of sensor loop

# The time values stored in the output file remain in UTC time zone

# Those output heart rates are going to require quality checking afterwards for
# the times when there was disagreement between the heart rate estimates

```


```{r reimportBPMfiles,echo=showcode}
# Import the algorithm-processed heart rate estimates for all of the 
# functional sensors and load them into data frames

# Controls, Trial 1:  SN173, SN196, SN224
# Low salinity Trial 1: SN189, SN218, SN179  (SN103 no data)

# Trial 2 control: SN214, SN242 (no data SN217, SN151)
# Trial2 # low salinity treatment: SN243, SN198, (no data SN245, SN226)

# Time values 

myBPMs = c('SN173','SN196','SN224','SN189','SN218','SN179','SN214','SN242',
		'SN243','SN198')


for (i in 1:length(myBPMs)){
	temp = read.csv(file = paste0(localpath,outputheartratedir,
					myBPMs[i],'_bpm.csv'))
	# Get start and stop time from trialMetaData 
	startTime = trialMetaData$TrialStartTimePDT[which(trialMetaData$SerialNumber ==
							myBPMs[i])]
	stopTime =  trialMetaData$TrialStopTimePDT[which(
					trialMetaData$SerialNumber == myBPMs[i])]
	
	# Convert DateTime into POSIXct format and shift to PDT time zone
	temp$DateTime = as.POSIXct(temp$DateTime, tz = 'UTC')
	attr(temp$DateTime, 'tzone') = 'PST8PDT'  # shift to Pacific Time
	# Trim the start and stop times
	temp = temp[which(temp$DateTime >= startTime),]
	temp = temp[which(temp$DateTime <= stopTime),]
	
	assign(paste0(myBPMs[i],'bpm'), value = temp)
}

```


```{r numberofNAheartrates}
# Show the number of heart rate readings that came back NA. Ideally this 
# number is small, but several of the mussels have a large number of readings
# where the algorithms couldn't agree for some reason or another. 
length(which(is.na(SN173bpm$finalBPM)))
length(which(is.na(SN196bpm$finalBPM)))
length(which(is.na(SN224bpm$finalBPM)))
length(which(is.na(SN189bpm$finalBPM)))
length(which(is.na(SN218bpm$finalBPM)))
length(which(is.na(SN179bpm$finalBPM)))
length(which(is.na(SN214bpm$finalBPM)))
length(which(is.na(SN242bpm$finalBPM)))
length(which(is.na(SN243bpm$finalBPM)))
length(which(is.na(SN198bpm$finalBPM)))

```

```{r interactivePlotFunctions}
####################################################################################
# Creating basic interactive graphing routines that allow a user to press a
# particular key to create some response

# 2 functions to allow user interaction with a graph
readkeygraph <- function(prompt)
{
	getGraphicsEvent(prompt = prompt, 
			onMouseDown = NULL, onMouseMove = NULL,
			onMouseUp = NULL, onKeybd = onKeybd,
			consolePrompt = "Enter a key:\npress n = next (enters NAs) \nk = keep current values \nr = refilter \nl = less filtering \np = use pracma bpm \nf = use forecast bpm \nb = step back to previous time \nq to quit")
	Sys.sleep(0.01)
	return(keyPressed)
}

onKeybd <- function(key)
{
	keyPressed <<- key
}
# End of function definitions

```


```{r QAvisualInspectionSetup, echo=showcode, eval = evalAll}
# Go through the results lists and visually inspect timepoints where the
# heart rate was flagged as questionable. This may be a one-time thing. Run the
# code in this chunk to open the BPM data and extract the data for your 
# desired sensor channel ('mysensor'). Then run the code in the next chunk
# to do the manual inspection. You may save your progress at the end of the
# next chunk (manually) so that it will be available here the next time you
# reopen the data file. 

# Controls, Trial 1:  SN173, SN196, SN224
# Low salinity Trial 1: SN189, SN218, SN179  (SN103 no data)

# Trial 2 control: SN214, SN242 (no data SN217, SN151)
# Trial2 # low salinity treatment: SN243, SN198, (no data SN245, SN226)

mysensor = 'IR'

mySN = 'SN218'

# Open up the concatenated raw heart data for this mussel
Heart = read.csv(paste0(localpath,outputheartdir,
				mySN,'_heart.csv'))
Heart$DateTime = as.POSIXct(Heart$DateTime, tz = 'UTC', format="%Y-%m-%d %H:%M:%OS")
# For the time being we will leave the time stamps in UTC

# Open up the output file from the heart rate auto-processing routine
if (length(dir(path = paste0(localpath,outputheartratedir), 
				pattern = paste0(mySN,'[[:alnum:][:punct:]]*QA.csv'))) == 1){
	# If the QA'd version exists already, load that up
	Heartbpm = read.csv(paste0(localpath,outputheartratedir, mySN,'_bpm_QA.csv'))
} else {
	# Reopen the original filtered results file, which hasn't had QA started yet
	Heartbpm = read.csv(paste0(localpath,outputheartratedir,mySN,'_bpm.csv'))	
}
# Convert DateTime to POSIX
Heartbpm$DateTime = as.POSIXct(Heartbpm$DateTime, tz = 'UTC', format="%Y-%m-%d %H:%M:%S")

# Get start and stop time from trialMetaData 
startTime = trialMetaData$TrialStartTimePDT[which(trialMetaData$SerialNumber ==
						mySN)]
stopTime =  trialMetaData$TrialStopTimePDT[which(
				trialMetaData$SerialNumber == mySN)]

startTimeUTC = lubridate::with_tz(startTime, tzone = 'UTC')
stopTimeUTC = lubridate::with_tz(stopTime, tzone = 'UTC')

# Trim the start and stop times
Heartbpm = Heartbpm[which(Heartbpm$DateTime >= startTimeUTC),]
Heartbpm = Heartbpm[which(Heartbpm$DateTime <= stopTimeUTC),]





# The heart beat estimates and quality flags are stored in Heartbpm


# Define a BPM quality threshold. The BPM estimates from the various methods should 
# be less than this value, meaning they are in close agreement. 
BPMqualityThreshold = 4

# Figure out rows where there's disagreement among the 3 methods and also values haven't
# previously been QA'd. If the row already has been QA'd, this will ignore that row. 
# This will pull out any row where the disagreement
# between 2 of the methods is greater than the BPMqualityThreshold
# and the sensor hasn't been QA'd. This will catch rows with 
# weak signals though, which may be too many.
checkTheseRows = which( (Heartbpm$BPMagreeQuality.fft.forecast > BPMqualityThreshold |
			Heartbpm$BPMagreeQuality.prac.forecast > BPMqualityThreshold |
		Heartbpm$BPMagreeQuality.prac.fft > BPMqualityThreshold ) &
	Heartbpm$QA == 'FALSE')
# give every row where it's not all NAs, include good HR estimates and ones that
# need to be QAd
# checkTheseRows = which( (!is.na(Heartbpm$BPMfft)) & (!is.na(Heartbpm$BPMforecast)))

# This version requires the WeakSignalFlag field to be OK instead
# of FAIL. Useful in cases where large numbers (thousands) of 
# rows would be caught here, but most of them have such a weak signal
# that they're not even worth looking at. 
#checkTheseRows = which( (sensor$BPMagreeQuality.fft.forecast > BPMqualityThreshold | 
#				sensor$BPMagreeQuality.prac.forecast > BPMqualityThreshold | 
#				sensor$BPMagreeQuality.prac.fft > BPMqualityThreshold ) & 
#			sensor$QA == 'FALSE' &
#			  sensor$WeakSignalFlag == 'OK')
# Also get the rows with NAs
NArows = which(is.na(Heartbpm$BPMfft) & is.na(Heartbpm$BPMforecast))
# Combine the two sets of bad row indices
badRows = c(checkTheseRows,NArows)

counter = 1 # Start at the first entry in checkTheseRows

if (exists('ManualStart')){
	# This won't run automatically, but you can use it to find a starting point in the middle 
	# of the dataset to start at, if you don't want to start at the first entry
	myTime = as.POSIXct('2022-08-22 10:52:30', tz = 'etc/GMT+8')
	counter = which.min(abs(myTime - Heartbpm$DateTime[checkTheseRows]))
}

```

```{r QAmanualVisualInspection,echo=showcode,eval=evalAll}
# On a Mac in Rstudio, before you run this chunk, run the following on the command line:
#X11(type='Xlib')

# On a Windows machine in Rstudio, you may instead need to run this line:
# dev.new()

# The goal is to open a plotting window outside of Rstudio that can accept user input

# The value 'counter' should be defined in the previous chunk

quitFlag = FALSE
skipFlag = FALSE
ylims = c(-300,300)
ylims=NULL
amplitudeThresh = 10
chunkLen = 400
myFs = 0.125

# You should have 
sensor = Heartbpm  # make a copy of the original data to work on


while (!quitFlag){
	# Get the row index of the next questionable row
	thisrow = checkTheseRows[counter]
	
	# Get the time stamp of the current row
	tstamp = sensor$DateTime[thisrow]
	currentBPM=sensor$finalBPM[thisrow]
	HighBandDenom=sensor$finalfilter[thisrow]
	QAd=sensor$QA[thisrow]
	t1 = which.min(abs(tstamp - Heart$DateTime))
	# Also get the rows for the sample periods before and after the target minute
	t0 = which.min(abs( (tstamp-120) - Heart$DateTime))
	t2 = which.min(abs( (tstamp+120) - Heart$DateTime))

	# Make the first version of the plot
	op = par()
	par(mfcol = c(1,3), mar = c(5,5,5,1))
	# Plot the prior minute's data for context
	junk = getBPM(Heart,t1 = t0, sensor = mysensor, chunklength = chunkLen,
			lowband = 1/200, highband = 1/HighBandDenom, amplitudeThreshold = amplitudeThresh,
			maxBPM = 50, Fs = myFs, plot = TRUE, myYlims = ylims)
	if (is.na(junk$BPMfft) & is.na(junk$BPMforecast) & is.na(junk$BPMpeaks)){
		# If all of these were NA, there's nothing to plot
		plot.new()
		box()
		text(x=0.5,y=0.5, labels = 'No good data')
	} 
	# Plot the one we're interested in
	res1 = getBPM(Heart,t1 = t1, sensor = mysensor, chunklength = chunkLen,
			lowband = 1/100, highband = 1/HighBandDenom, amplitudeThreshold = amplitudeThresh,
			maxBPM = 50, Fs = myFs, plot = TRUE, myYlims = ylims)
	#Show currently recorded final BPM (may be NA)
	mtext(side=3, line=-2, text=paste("Recorded BPM: ", currentBPM), adj=0)
	#show if previous chunk was QAd 
	mtext(side=3, line=-3, text=paste("Previous QA: ", ifelse(QAd==TRUE, "TRUE", "FALSE")), adj=0)
	# Sanity check, if nothing gets plotted then we need to skip
	if (is.na(res1$BPMfft) & is.na(res1$BPMforecast) & is.na(res1$BPMpeaks)){
		# If all of these were NA, there's nothing to plot
		plot.new()
		box()
		text(x=0.5,y=0.5, labels = 'No good data')
		skipFlag = TRUE
	} 
	mtext(side = 1, text = paste0(counter,'/',length(checkTheseRows)), adj = 0, line = 2.5)
	# Plot the next minute's data as well, just for context
	junk = getBPM(Heart,t1 = t2, sensor = mysensor, chunklength = chunkLen,
			lowband = 1/100, highband = 1/HighBandDenom, amplitudeThreshold = amplitudeThresh,
			maxBPM = 50, Fs = myFs, plot = TRUE, myYlims = ylims)
	if (is.na(junk$BPMfft) & is.na(junk$BPMforecast) & is.na(junk$BPMpeaks)){
		# If all of these were NA, there's nothing to plot
		plot.new()
		box()
		text(x=0.5,y=0.5, labels = 'No good data')
	} 
	
	keyPressed = '' # initialize this so that it exists
	while (keyPressed != 'q'){
		# First, if the data for the desired time had too many missing values
	  # and skipFlag is true, handle that case and skip over this time point
		if (skipFlag){
			skipFlag = FALSE
			next
		}
		
		# Use the readkeygraph() function to prompt the user for input on the graph
		keyPressed = readkeygraph(prompt = "Enter a key: n = next (enters NAs), k = keep [average] 3 BPM values, r = refilter, l = less filtering, p = use pracma bpm, f = use fft bpm, b = step back to previous time, 0 = flatline signal, s = save and quit, q = quit without saving")
		keyPressed # print the resulting key press, this will be a character value
		# At this point interpret the resulting key press and cause some
		# decision to be made 
		if (keyPressed == 'n'){
			# User typed n, set the BPM values and flags to NA, this chunk isn't usable
			sensor[thisrow,3:8] = NA
			sensor$finalBPM[thisrow] = NA
			sensor$QA[thisrow] = TRUE
			keyPressed = 'q' # break out of the while loop
		} else if (keyPressed == 'k') {
			# User typed k, keep the current values and calculate a final BPM value
			sensor$finalBPM[thisrow] = round(mean(res1$BPMfft, res1$BPMforecast,res1$BPMpeaks),1)
			sensor$QA[thisrow] = TRUE
			# Store the final highband filter value used as well
			sensor$finalfilter[thisrow] = HighBandDenom
			keyPressed = 'q'  # Set to q to quit this round of the while loop
		} else if (keyPressed == 'r') {
			# Refilter more aggressively
			HighBandDenom = HighBandDenom + 5 
			junk = getBPM(Heart,t1 = t0, sensor = mysensor, chunklength = chunkLen,
					lowband = 1/100, highband = 1/HighBandDenom, amplitudeThreshold = amplitudeThresh,
					maxBPM = 80, Fs = myFs, plot = TRUE, myYlims = ylims)
			res1 = getBPM(Heart,t1 = t1, sensor = mysensor, chunklength = chunkLen,
					lowband = 1/100, highband = 1/HighBandDenom, amplitudeThreshold = amplitudeThresh,
					maxBPM = 80, Fs = myFs, plot = TRUE, myYlims = ylims)
			junk = getBPM(Heart,t1 = t2, sensor = mysensor, chunklength = chunkLen,
					lowband = 1/100, highband = 1/HighBandDenom, amplitudeThreshold = amplitudeThresh,
					maxBPM = 80, Fs = myFs, plot = TRUE, myYlims = ylims)
			keypressed = ''
			# now we return to the top of the while loop
		} else if (keyPressed == 'l') {
			# Filter less aggressively
		  if (HighBandDenom > 5){
		    	HighBandDenom = HighBandDenom - 5
		  } else if (HighBandDenom <= 5 & HighBandDenom > 3) {
		     HighBandDenom = HighBandDenom - 1
		  } else if (HighBandDenom <= 3){
		    HighBandDenom = 3
		  }

			junk = getBPM(Heart,t1 = t0, sensor = mysensor, chunklength = chunkLen,
					lowband = 1/100, highband = 1/HighBandDenom, amplitudeThreshold = amplitudeThresh,
					maxBPM = 50, Fs = myFs, plot = TRUE, myYlims = ylims)
			res1 = getBPM(Heart,t1 = t1, sensor = mysensor, chunklength = chunkLen,
					lowband = 1/100, highband = 1/HighBandDenom, amplitudeThreshold = amplitudeThresh,
					maxBPM = 50, Fs = myFs, plot = TRUE, myYlims = ylims)
			junk = getBPM(Heart,t1 = t2, sensor = mysensor, chunklength = chunkLen,
					lowband = 1/100, highband = 1/HighBandDenom, amplitudeThreshold = amplitudeThresh,
					maxBPM = 50, Fs = myFs, plot = TRUE, myYlims = ylims)
			keypressed = ''
			# Now we return to the top of the while loop
		} else if (keyPressed == 'p') {
			# User wants to assign the pracma-derived BPM value as the finalBPM
			sensor$finalBPM[thisrow] = res1$BPMpeaks
			# Store the final highband filter value used as well
			sensor$finalfilter[thisrow] = HighBandDenom
			sensor$QA[thisrow] = TRUE
			keyPressed = 'q' # set this to move on to next time point
		} else if (keyPressed == 'f') {
			# User wants to use the fft-derived BPM value as the finalBPM
			sensor$finalBPM[thisrow] = res1$BPMfft
			# Store the final highband filter value used as well
			sensor$finalfilter[thisrow] = HighBandDenom	
			sensor$QA[thisrow] = TRUE
			keyPressed = 'q' # set this to move to next time point
		} else if (keyPressed == 'b') {
			# User wants to go back to the previous plot, maybe they want to do that
			# one over again.
			counter = counter - 2 # Decrement by 2, because the end of the loop will increment by 1 again
			keyPressed = 'q' # Set to q to exit while loop			
		} else if (keyPressed == 'q'){
			quitFlag = TRUE # Set TRUE to exit outer while loop
		} else if (keyPressed == 's') {
			# Write the updated data back into the main data frame
			Heartbpm[which(Heartbpm$Sensor == mysensor),] = sensor
			# Save the data frame to a csv file
			write.csv(Heartbpm, file = paste0(heartoutputpath,oy,'_bpm_QA.csv'), 
					row.names=FALSE)
			# User wants to save progress and quit
			keyPressed = 'q' # Set to q to exit while loop
			quitFlag = TRUE # Set TRUE to exit outer while loop
		} else if (keyPressed == '0'){
		  sensor$finalBPM[thisrow] = 0
		  sensor$finalfilter[thisrow] = HighBandDenom
		  sensor$QA = TRUE
		  keyPressed = 'q' # Set to q to exit while loop
		} else {
			# Key pressed wasn't one of the choices above, so do some default action
			mtext(side = 3, line = -2, text = "No sensible choice made")
		}
	}
	keyPressed = '' # reset the value
	counter = counter+1 # increment the counter
	if(counter > length(checkTheseRows)){
	  quitFlag = TRUE
	}
	par(op)
	
	
}

print(paste('Last row checked: ', checkTheseRows[i], ' time: ', sensor$DateTime[thisrow]))

# At the end of the loop, the 'sensor' data frame should have the 'finalBPM' values
# assigned in most rows (besides rows with bad data)
	

# userInput = readline(prompt = "Save progress? y or n: \n")
userInput = askYesNo(msg = "Save progress? y or n: \n")

if(userInput){
	# Write the updated data back into the main data frame
	Heartbpm = sensor
	# Save the data frame to a csv file
	write.csv(Heartbpm, file = paste0(localpath,outputheartratedir,mySN,'_bpm_QA.csv'), 
			row.names=FALSE)
} else {
  print("Updates not saved.")
}

```


```{r openQAdHeartRate,echo=showcode}

myBPMs = c('SN173','SN196','SN224','SN189','SN218','SN179','SN214','SN242',
		'SN243','SN198')


for (i in 1:length(myBPMs)){
	if (length(dir(path = paste0(localpath,outputheartratedir), 
					pattern = paste0(myBPMs[i],'[[:alnum:][:punct:]]*QA.csv'))) == 1){
		temp = read.csv(file = paste0(localpath,outputheartratedir,
						myBPMs[i],'_bpm_QA.csv'))
		# Get start and stop time from trialMetaData 
		startTime = trialMetaData$TrialStartTimePDT[which(trialMetaData$SerialNumber ==
								myBPMs[i])]
		stopTime =  trialMetaData$TrialStopTimePDT[which(
						trialMetaData$SerialNumber == myBPMs[i])]
		
		# Convert DateTime into POSIXct format and shift to PDT time zone
		temp$DateTime = as.POSIXct(temp$DateTime, tz = 'UTC')
		attr(temp$DateTime, 'tzone') = 'PST8PDT'  # shift to Pacific Time
		# Trim the start and stop times
		temp = temp[which(temp$DateTime >= startTime),]
		temp = temp[which(temp$DateTime <= stopTime),]
		
		assign(paste0(myBPMs[i],'bpmQA'), value = temp)
	}
}


```

```{r testPlot,echo=showcode}
# Controls, Trial 1:  SN173, SN196, SN224
# Low salinity Trial 1: SN189, SN218, SN179  (SN103 no data)

# Trial 2 control: SN214, SN242 (no data SN217, SN151)
# Trial2 # low salinity treatment: SN243, SN198, (no data SN245, SN226)

par(mfrow=c(3,1))
# Plot the QA'd heart rate data and gape data for SN173 in Trial 1 (Control)
# Generally had good well-defined heart signal with high amplitude, although
# there were some sections that seemed arhythmic and had to be scored NA
plot(SN173bpmQA$DateTime, SN173bpmQA$finalBPM, type = 'l', 
		ylim = c(0,30), las = 1)
lines(SN173gape$DateTime, SN173gape$gape.mm, type = 'l', col = 'blue')
legend('topleft',legend=c('Heart rate','Gape, mm'), col = c('black','blue'),
		lty= c(1,1))
title('SN173 Control')

plot(SN189bpmQA$DateTime, SN189bpmQA$finalBPM, type = 'l', col = 2,
		ylim = c(0,30), las = 1)
lines(SN189gape$DateTime, SN189gape$gape.mm, type = 'l', col =3)
legend('topleft',legend=c('Heart rate','Gape, mm'), col = c(2,3),
		lty= c(1,1))
title('SN189 Low Salinity')

# SN218 had a relatively weak signal and therefore a lot of readings couldn't
# be positively assigned a heart rate. There are periods during the 
# low salinity section where it appeared that it was only producing 1 beat 
# irregularly during the 50 seconds and then staying mostly flat the rest of the 
# time, and this single beat would repeat in each of the subsequent 2 minute 
# interval samples. So I'm almost willing to say that it was doing 1 beat per
# minute for a long time while closed, but I had to assign NAs to those periods
plot(SN218bpmQA$DateTime, SN218bpmQA$finalBPM, type = 'l', col = 4,
		ylim = c(0,30), las = 1)
lines(SN218gape$DateTime, SN218gape$gape.mm, type = 'l', col =5)
legend('topleft',legend=c('Heart rate','Gape, mm'), col = c(4,5),
		lty= c(1,1))
title('SN218 Low Salinity')


```