---
title: "LPL lab mussel salinity trials"
output: html_document
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---


# TODOs

* Write code to import all gape files from a particular mussel (SN###) and
  concatenate them into a single file

* Process those concatenated data to convert raw gape values into estimates of
gape opening %

* Write code to import and concatenate heart data files for each mussel

* Write code to process raw heart data into beats per minute estimates, with
manual quality assurance as needed

* Other stuff


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
showcode = FALSE # Set TRUE to print out all code in the output document
evalAll = FALSE # Set TRUE to run all chunks including the original data 
# ingestion, which can take 30 minutes+. 
options(digits.secs = 3) # Necessary to deal with sub-second time stamps in 
# heart data
```

# Libraries
```{r loadLibraries, echo=TRUE, message=FALSE}
library(tidyverse)
library(readxl)

# This will also use the libraries signal, pracma, and forecast, but will
# only load them as needed to avoid masking functions from other libraries

#library(scales)
#library(readr) 
#library(dplyr)
#library(Tides)
#library(janitor)
#library(zoo)
#library(esquisse)
#library(timetk)
#library(ggpattern)
```


# Setup

Define paths to files and create functions to handle data

```{r fileLocations, echo=FALSE}
# Path from the R code directory to the data directory
localpath = '../../data/Lab_mussel_salinity_trials/LPL_Mytilus_lab_salinity_trials_Summer2023/'  # Luke's path from the R cod dir to the data dir

# Path to the folder with the raw data subfolders
trialpath = 'Raw_data_files_Summer2023/Trial_data_Aug-Sep_2023/'

# Path to directory to hold concatenated gape files
outputgapedir = 'Concatenated_gape_files/'

# Path to directory to hold concatenated heart files
outputheartdir = 'Concatenated_heart_files/'

# Path to directory to hold processed estimated heart rates
outputheartratedir = 'Heart_rate_estimates/'

# Path to the metadata file
metafilename = 'Mussel_metadata_calibs_Summer2023.xlsx'

# File containing the trial salinity/temperature/dissolved oxygen data
trialfilename = 'Mussel_salinity_temp_data_Summer2023.xlsx'

```


```{r gapeFunctions,echo=FALSE}


#' HalltoMM function - use this to convert raw Hall effect values to estimates
#' of gape opening distance (mm), using calibration data in the CalibFits data
#' frame
#' 
#' @param GapeData A data frame containing a column of raw Hall values and a 
#' column called 'SN' containing the data logger serial number
#' @param CalibFits A data frame containing a column called 'SerialNumber' that
#' is used to match up with the SN value in the GapeData data frame, along with
#' a column 'Intercept' and a column 'Slope' containing coefficients from a
#' linear regression of Hall readings vs. known gape distance
#' 
#' @return Adds a column called gape.mm into the GapeData data frame with the 
#' estimated gape distance in mm. Values of gape that calculate out to less
#' than zero are truncated to zero millimeters.

HalltoMM = function(GapeData, CalibFits) {
	# Find the data in CalibFits that match the serialnumber, and estimate the
	# gape opening in millimeters using the stored intercept and slope values
	# in CalibFits.
	GapeData$gape.mm = (GapeData$Hall * 
				CalibFits$Slope[which(CalibFits$SerialNumber == GapeData$SN[1])]) + 
			CalibFits$Intercept[which(CalibFits$SerialNumber == GapeData$SN[1])]
	# Truncate any values below zero
	GapeData$gape.mm[which(GapeData$gape.mm < 0)] = 0
	 # Return the input data frame with its new data column
	GapeData
}

#####################################################################
#' calcPercentGape function
#' 
#' This function assumes that the input values are already converted from raw
#' Hall sensor values to estimates of gape distance in millimeters. It returns
#' a vector of gape values expressed as a percentage of the fully open distance
#' in the range 0-100%.
#' 
#' @param gape.mm A vector of shell gape values measured in millimeters
#' 
#' @return A vector of gape values converted to a percentage opening (0-100%) 

calcPercentGape <- function(gape.mm){
	# Convert to percentage by dividing all distances by the maximum value
	outputData = (gape.mm / max(gape.mm,na.rm=TRUE)) * 100
	
	# Round the values off to a reasonable precision
	outputData = round(outputData,1)
	
	# Return the vector of percentage values (0-100%)
	outputData
}


#TODO: the DistFromCounts, percentileRange, and calcPercentGapeNLME 
# functions all need to be updated to make use of the more linear
# calibration data for these mussels

# A set of functions originally taken from MusselTracker_data_proc4.R to 
# handle the gape data

################################################################################
### gapBounds function
# A function to determine where there are gaps of missing data, so that we can 
# apply a filter to the good chunks of data. Most filters fail if there are NAs
# present, and our remaining gaps in the data are big enough that they probably
# shouldn't be smoothed-over anyhow. 
#' A function to find the start and end indices of numeric data in a vector. 
#' Useful for dealing with timeseries that have gaps, and operations that need
#' to operate the separate chunks of contiguous numeric values
#' @param value A vector of numeric values, with NAs 
#' @return A data frame with 2 columns, Start and End, which contain the row
#' indices for the first good and last good values in each contiguous run of
#' numeric values. 	
gapBounds <- function (values){
	# This function returns a data frame of 2 columns, start + end, that give
	# the row indices of the start and end of each run of good data in the 
	# input vector. It should handle data streams that start with NAs and
	# should handle streams that end with NAs. 
	
	# Use run length encoding function to find NA gaps
	gaps = rle(is.na(values))
	# If the first entry in gaps is TRUE, this indicates that the data started
	# with NA values.
	if (gaps$values[1] == TRUE) {
		startNA = TRUE	# Data started with NAs
	} else {	
		startNA = FALSE	 # Data started with real values
	}
	# If the last entry in gaps is TRUE, the data end on NAs
	if (gaps$values[length(gaps$values)] == TRUE){
		endNA = TRUE	# Data ends with NAs
	} else {
		endNA = FALSE	# Data ends with real values
	}
	
	# The number of gaps with value == TRUE is the number of good runs of data
	# and gaps with value == FALSE are the starts of runs of NAs. This will
	# get the number of FALSE (negated to TRUE) values in gaps. A dataset
	# ending in good data will have a last entry in gaps of FALSE. 
	numgaps = sum(!gaps$values) 
	
	# Create the output data frame
	results = data.frame(Start = integer(numgaps), End = integer(numgaps))
	
	if (!startNA) {
		# If startNA == FALSE (started on good data, the first entry should 
		# always be 1 (1st row) 
		# If there are no gaps, the
		# contents of gaps$lengths will just be the length of the values vector
		results$Start[1] = 1
		results$End[1] = gaps$lengths[1]
	} else if (startNA){
		# However, if the dataset starts with NAs, the first entry in gaps will
		# be the index of the first good data, while the 2nd entry will be the 
		# start of the next stretch of NAs
		results$Start[1] = gaps$lengths[1]+1
		results$End[1] = sum(gaps$lengths[1:2])
	}
	
	# If there is more than 1 entry in gaps$lengths, process the other gaps
	j = 2; # counter
	if (numgaps > 1){
		if (!startNA & endNA){
			# If the dataset ends on NAs (TRUE last), truncate gapind by 1
			gapind = seq(2,length(gaps$lengths)-1, by = 2)
		} else if (!startNA & !endNA) {
			# If the dataset ends on good values (FALSE last)
			gapind = seq(2,length(gaps$lengths), by = 2)
		} else if (startNA & endNA) {
			# If dataset starts on NAs (TRUE 1st) and ends on NAs (TRUE last)
			gapind = seq(3, length(gaps$lengths)-1, by = 2)
		} else if (startNA & !endNA) {
			# If dataset starts on NAs (TRUE 1st) and ends on good data 
			# (FALSE last)
			gapind = seq(3, length(gaps$lengths), by = 2)
		}	
		# Step through the rest of the gaps object to find the start and end
		# points of each stretch of good data. 
		for (i in gapind){
			nextstart = sum(gaps$lengths[1:i]) + 1
			nextend = sum(gaps$lengths[1:(i+1)])
			results$Start[j] = nextstart
			results$End[j] = nextend
			j = j + 1
		}
	} # end of if (numgaps > 1)
	results	# return the results dataframe, 2 columns Start and End
}

################################################################################
### gapBounds2 function
#' Function that sets bounds for upper and lower limit of hall counts to 
#' calculate gape opening % while dealing with growth rates. Change chunk 
#' length to certain number of days (hall count rows) (arbitrary 5 days). Use 
#' gapbounds function to set new ranges. 
#' @param mygaps A data frame from gapbounds function
#' @param maxrow Max length of rows (time) to subset out 

gapBounds2=function(mygaps, maxrow){
	mydiff=mygaps$End[1]-mygaps$Start[1]
	if (mydiff>maxrow){
		quotient=mydiff/maxrow #quotient=number of subsets (chunks) to make based on max row
		nloops = floor(quotient)
		
		for (i in 1:nloops){
			if (i == 1 & nloops>=2){
				mygaps2 = data.frame(Start = mygaps$Start[1], End = mygaps$Start[1]+maxrow)
			} else if (i==1 & nloops<2){
				mygaps2 = data.frame(Start = mygaps$Start[1], End = mygaps$End[1])
			} else if (i > 1 & i < nloops){
				temp = data.frame(Start = mygaps2$End[i-1]+1, End = mygaps2$End[i-1]+maxrow )
				mygaps2 = rbind(mygaps2, temp)
			} else if (i > 1 & i == nloops){
				temp = data.frame(Start = mygaps2$End[i-1]+1, End = mygaps$End[1])
				mygaps2 = rbind(mygaps2, temp)
			}
		}
	} else if (mydiff <= maxrow){
		# Handle the case where the 1st entry in mygaps is shorter than maxrow, just copy the values
		# over to a dataframe we're calling mygaps2
		mygaps2 = data.frame(Start = mygaps$Start[1], End = mygaps$End[1])
	}
	# Handle cases where there is more than a single row in mygaps
	if (nrow(mygaps) > 1) {
		for (j in 2:nrow(mygaps)){
			mydiff=mygaps$End[j]-mygaps$Start[j]
			if (mydiff>maxrow){
				quotient=mydiff/maxrow #quotient=number of subsets (chunks) to make based on max row
				nloops = floor(quotient)
				
				for (i in 1:nloops){
					if (i == 1){
						temp = data.frame(Start = mygaps$Start[j], End = mygaps$Start[j]+maxrow)
						mygaps2 = rbind(mygaps2, temp)
					} else if (i > 1 & i < nloops){
						temp = data.frame(Start = mygaps2$End[nrow(mygaps2)]+1, End = mygaps2$End[nrow(mygaps2)]+maxrow)
						mygaps2 = rbind(mygaps2, temp)
					} else if (i == nloops){
						temp = data.frame(Start = mygaps2$End[nrow(mygaps2)]+1, End = mygaps$End[j])
						mygaps2 = rbind(mygaps2, temp)
					}
				}
			} else {
				# handle the case where the next entry in mygaps is shorter than maxrow, just copy the start/end from 
				# mygaps into mygaps2
				mygaps2 = rbind(mygaps2, mygaps[j,])
			}
		}
	}
	mygaps2 # return this data frame at the end of the function
	
}



################################################################################
# This filter function probably isn't necessary for the lab salinity trials 
# since they are collected at much lower frequency (1 minute) compared to the
# original 1-second sampling regime used back in 2015

### hallFilter function
#' A function to apply a Butterworth 1st order low-pass filter to a vector
#'  of data. If there are NA values in the original data, they will be 
#'  preserved in the output vector. The filter is set to filter at 1/10 the
#' original sampling rate (which is 5 sec interval (0.2Hz), producing an approximate
#' 50-second window of smoothing. Raw input data are initially centered on 0
#' to avoid artifacts at the start of the filter, and the output is re-centered
#' at the original starting value after the filtering is applied. 
#' 
#' @param hallData A vector of hall effect data.
#' @return A vector of filtered hall effect sensor data.
#'  
hallFilter = function(hallData){	
	# Define a butterworth low-pass filter, 1st order, to filter at 1/10 the 
	# sampling rate (which was 5 secs (0.2Hz))
	# myfilter = signal:::butter(1,0.1,type='low', plane = 'z')
	# Find the start and end of any gaps in the data set using the gapBounds
	# function defined earlier
	mygaps = gapBounds(hallData)
	# Apply the filter to the good runs of data
	for (i in 1:nrow(mygaps)){
		# Extract the run of good data
		dats = hallData[mygaps$Start[i]:mygaps$End[i]]
		# Find the offset of the data from zero
		offset = dats[1]
		# Subtract the offset off of all of the data in this run
		# (The butterworth filter returns large transient values at the start
		# of the run if the value is much different from zero)
		dats = dats - offset
		# Call the filter routine to apply the filter
		yfiltered = signal:::filter(myfilter,dats)
		# Add the offset back on so the data are back on their original scale
		yfiltered = yfiltered + offset
		# Write the filtered data back into the data vector
		hallData[mygaps$Start[i]:mygaps$End[i]] = yfiltered
	}
	# Round the filtered data back to the nearest whole number, since these
	# represent ADC count data.
	hallData = round(hallData)
	hallData	# return vector of filtered data
}



####{r DistFromCounts_function}
################################################################################
# LPM: This nonlinear approximation is not likely to be necessary for the
# calibrated Mytilus used in the lab salinity trials, since their opening range is 
# small enough that a simple linear fit is a sufficient approximation of 
# gape opening distance. 

### DistFromCounts function
# Define a non-linear function that allows you to plug in a Hall effect sensor
# count value and generate an approximate distance in mm. Supply the 
# coefficients a, b, c, based on a nls curve fit for counts as a function of
# distance. 
# In the above equation, 'a' is the effective asymptotic value
# 'b' is the range of values between the minimum observed count and the 
# asymptotic count (so a min count of 400 and asymptote of 500 gives b = 100)
# 'c' is effectively a curvature parameter. Larger values of 'c' yield a 
# smaller change in Distance as Counts increases (when back-calculating 
# Distance), particularly for low Counts values. 
#' A function to back-calculate an asymptotic function to generate a distance
#' value based on a hall effect sensor count value input.
#' 
#' @param Counts A vector of hall effect sensor values, usually between 0 and 
#' 512 (from a 10-bit analog-to-digital converter). 
#' @param a A parameter derived from an asymptotic curve fit, representing the
#' asymptotic count value
#' @param b A parameter derived from an asymptotic curve fit, representing the 
#' range from the minimum count up to the asymptotic value. 
#' @param c A parameter derived from an asymptotic curve fit, representing the
#' curvature of the fit. 
#' @returns A vector of distance values of the same length as Counts. Any input 
#' values equal or larger than the asymptotic parameter a will return Inf.

DistFromCounts <- function(Counts, a,b,c){
	dist = (log((Counts - a)/(-1*b))) / (-1*c)
	dist
}


###{r percentileRange_function}
################################################################################
### percentileRange function
# Define a function to calculate the average reading for the upper and lower
# x percentile of the hall effect data. This is designed to remove potential 
# outliers that may be due to disturbances such as another magnet getting near
# the sensor temporarily

#' Calculate the average of the specified lower percentile and upper percentile
#' and return the values that correspond to those means (rounded to integers).
#' 
#' @param hallData A vector of analog-to-digital convertor counts from a 
#' Hall effect sensor. The values are assumed to start low (<512) when the 
#' magnetic signal is strong (close to sensor) and increase towards 512 when
#' the magnetic signal is weak (far from sensor).
#' @param percentileLim A set of 2 numeric values between 0 and 1, usually close
#'  to 0.01 and 0.99
#' used to define the lower and upper percentile limits. A value of 
#' 0.01 would cause the 1st percentile sensor values to be used as the
#' lower limits, and similar for the upper limit. 
#' @return A two element vector for the lower and upper values in hallData that
#' represent the lower and upper percentiles

percentileRange <- function (Hallvec, percentileLim = c(0.01,0.99)){
	# Remove any NA's
	temp = Hallvec[!is.na(Hallvec)]
	# Reorder the values smallest to largest
	temp = temp[order(temp)]
	# Get the index of the entry closest to percentileLim
	indx = round(percentileLim[1] * length(temp)) 
	# Calculate the mean value for the lower % of closed valve values and
	# round up to the next integer value
	closedVal = ceiling(mean(temp[1:indx]))
	
	# Now do the same for the other end of the range of hall effect values
	# These would normally represent "fully open" readings near 512 if the 
	# magnet and sensor are situated so that the magnet drives the signal
	# below 512 when it approaches the sensor as the shell valves close. 
	indx = round(percentileLim[2] * length(temp))
	# Calculate the mean value for the upper % of open valve values and round
	# down to the next integer value
	openVal = floor(mean(temp[indx:length(temp)]))
	result = c(closedVal, openVal)
	result # return the two values, always smallest then largest
}





####{r calcPercentGapeNLME_function}
################################################################################
### calcPercentGapeNLME function
# Use this function to calculate the percentage gape (0-100%)
# This calculates the baseline fully-closed and fully-open values based on the upper and lower 1% of hall effect sensor data for the given range of rows in the input dataset. Supply a vector of Hall effect sensor data. This function will find any gaps in the vector (NA's) and calculate a new baseline for each section of good data. 
#' Convert hall effect count data (integer values from analog-digital converter) into percent gape values between 0 and 100\%. ' 
#' @param hallData A vector of analog-to-digital convertor counts from a  Hall effect sensor. The values are assumed to start low (<512) when the magnetic signal is strong (close to sensor) and increase towards 512 when the magnetic signal is weak (far from sensor).
#' @param cRegress A linear model object (lm) for the relationship of 'c' versus the log-transform of 'b'. 
#' @param percentileLim A numeric value between 0 and 1, usually close to 0.01, used to define the lower and upper percentile limits (the upper limit is mirrored from the lower limit). A value of 0.01 would cause the 1st and 99th percentile sensor values to be used as the lower and upper limits.
#' @return A vector of the same length as hallData containing percentage values 0 to 100, representing the shell valve gape opening. A value of 0 is a fully closed shell (strong magnetic signal) and 100 is a fully open shell (weak magnetic signal).

calcPercentGapeNLME <- function (hallData, cRegress, percentileLim = 0.01){
	require(nlme)
	# Get the row indices for the major gaps that exist now
	mygaps = gapBounds(hallData)
	# Create an empty output vector of the same length as the input data. 
	outputData = vector(mode = 'numeric', length = length(hallData))
	outputData[] = NA # Convert all the values to NA to begin with
	
	# Now for each section of the input data, calculate the percent gape. This involves using the entries in mygaps to subset the input data and calculate individual percentile values for each contiguous section of data. If there is a significant gap in the data (usually > 30 sec), the percentages will be re-calculated for each contiguous section of data. This should accommodate any sensor/magnet re-glueing issues. 
	for (i in 1:nrow(mygaps)){
		st = mygaps$Start[i]
		end = mygaps$End[i]
		# First calculate the upper and lower 1% values of the Hall readings. These will be count values representing the fully closed (lowest) and fully open (highest, closest to 512) values.
		myrange = percentileRange(hallData[st:end], 
				percentileLim = percentileLim)
		
		# Now truncate count values that are outside myrange to the limits of myrange.
		rowIndices = which(hallData[st:end] < myrange[1])
		hallData[(st+rowIndices)] = myrange[1]
		rowIndices = which(hallData[st:end] > myrange[2])
		hallData[(st+rowIndices)] = myrange[2]
		
		# The count values are now constrained within a range that should encompass fully closed (0mm) to fully open (~5mm gape). Now calculate the approximate gape opening based on the count values and a curve fit. 
		
		# Define the three asymptotic curve parameters a,b,c using the data for the current chunk of data.
		aVal = max(hallData[st:end],na.rm=TRUE)+1 # add 1 count to avoid Inf output.
		bVal = max(hallData[st:end],na.rm=TRUE) - 
				min(hallData[st:end],na.rm=TRUE)
		# Calculating c requires the linear model object supplied as cRegress, which will use the log-transformed 'b' value to estimate the 'c' parameter
		cVal = predict(cRegress, newdata = list(b = bVal))
		
		# Use DistFromCounts function to estimate valve gape distance using the input hall effect count data at each time point and the parameters a,b,c
		temp2 = DistFromCounts( hallData[st:end], 
				a = aVal,
				b = bVal,
				c = cVal)
		
		# Replace any fully-open Inf values with the largest good max distance
		temp2[is.infinite(temp2)] = max(temp2[is.finite(temp2)])
		# Replace any negative distance values with the 0 distance value
		temp2[which(temp2 < 0)] = 0
		# Convert to percentage by dividing all distances by the maximum value
		outputData[st:end] = (temp2 / max(temp2,na.rm=TRUE)) * 100
		
		# Round the values off to a reasonable precision
		outputData[st:end] = round(outputData[st:end],1)
		
	}	
	outputData	# return the output data vector
}	# end of calcPercentGapeNLME function




####{r calcPercentGapeNLMEsubset_function}
################################################################################
### calcPercentGapeNLMEsubset function
# Use this function to calculate the percentage gape (0-100%) of only a subset of data (days) at a time to account for growth of oyster shell. 
# This calculates the baseline fully-closed and fully-open values based on the upper and lower 1% of hall effect sensor data for the given range of rows in the input dataset. Supply a vector of Hall effect sensor data. This function will find any gaps in the vector (NA's) and calculate a new baseline for each section of good data. 
#' Convert hall effect count data (integer values from analog-digital converter) into percent gape values between 0 and 100\%. ' 
#' @param hallData A vector of analog-to-digital converter counts from a  Hall effect sensor. The values are assumed to start low (<512) when the magnetic signal is strong (close to sensor) and increase towards 512 when the magnetic signal is weak (far from sensor).
#' @param cRegress A linear model object (lm) for the relationship of 'c' versus the log-transform of 'b'. 
#' @param percentileLim A numeric value between 0 and 1, usually close to 0.01, used to define the lower and upper percentile limits (the upper limit is mirrored from the lower limit). A value of 0.01 would cause the 1st and 99th percentile sensor values to be used as the lower and upper limits.
#' @param myPercentThreshold A value from 0 to 1 used during the final conversion from distance to 
#' percentage gape to filter all very wide values to just be 100%. You can put a value such as 0.9
#' and this will take any estimated distance value greater than the 90th percentile for the current
#' chunk of hall data and convert all of those large values to 100%. All of the smaller distance values
#' will also be scaled relative to this 90th percentile distance, meaning that smaller distances will 
#' appear as greater percentage opening values than they would if myPercentThreshold were set to the 
#' default 1.0. 
#' @return A vector of the same length as hallData containing percentage values 0 to 100, representing the shell valve gape opening. A value of 0 is a fully closed shell (strong magnetic signal) and 100 is a fully open shell (weak magnetic signal).
#' @param maxrow A number of rows used to define the upper and lower limits of hall sensor calculations. Chose arbitrary 7 day chunk. may want to be 5 days bc if have little bit of leftover chunk then can  add on to previous so then last chunk is 7 days. 
#' @param minrows The minimum number of rows of contiguous hall data that we will allow to be converted
#' to a percentage gape. Because very short chunks of hall data will probably not encompass the full 
#' range of opening and closing, we wouldn't want to try to calculate 0-100% open values. minrows will be
#' specified as a number of rows, so the conversion to length of time depends on how frequently the 
#' hall data were being sampled. Lauren's BivalveBit sensors were collecting once a minute. 1440 minutes
#' in a day

calcPercentGapeNLMEsubset <- function (hallData, cRegress, percentileLim = c(0.01,0.99), maxrow, minrows = 1440, myPercentThreshold = 1.0){
	require(nlme)
	if (exists('aValOld')) {rm(aValOld)}
	if (exists('oldthresholdvalue')) {rm(oldthresholdvalue)}
	# Get the row indices for the major gaps that exist now
	mygaps = gapBounds(hallData) #identify start and end of non-NA chunks of data.
	
	# Check if any of the Start/End values in mygaps are less than minrows apart. If they are, remove
	# that entry from mygaps
	mydiffs = mygaps$End - mygaps$Start
	# remove chunks that are too small 
	mygaps = mygaps[which(mydiffs > minrows),] 
	
	if (nrow(mygaps > 0)){
		# Use gapBounds2 to subdivide the continuous data into chunks of time that are defined by maxrow
		mygaps = gapBounds2(mygaps, maxrow)
		
		# Create an empty output vector of the same length as the input data. 
		outputData = vector(mode = 'numeric', length = length(hallData))
		outputData[] = NA # Convert all the values to NA to begin with
		
		# Now for each section of the input data, calculate the percent gape. This involves using the entries in mygaps to subset the input data and calculate individual percentile values for each contiguous section of data. If there is a significant gap in the data (usually > 30 sec), the percentages will be re-calculated for each contiguous section of data. This should accommodate any sensor/magnet re-glueing issues. 
		for (i in 1:nrow(mygaps)){
			st = mygaps$Start[i]
			end = mygaps$End[i]
			# First calculate the upper and lower 1% values of the Hall readings. These will be count values representing the fully closed (lowest) and fully open (highest, closest to 512) values.
			myrange = percentileRange(hallData[st:end], 
					percentileLim = percentileLim)
			
			# Now truncate count values that are outside myrange to the limits of myrange.
			rowIndices = which(hallData[st:end] < myrange[1])
			hallData[((st-1)+rowIndices)] = myrange[1]
			rowIndices = which(hallData[st:end] > myrange[2])
			hallData[((st-1)+rowIndices)] = myrange[2]
			
			# The count values are now constrained within a range that should encompass fully closed 
			# (0mm) to fully open (~5mm gape). Now calculate the approximate gape opening based on the count values and a curve fit. 
			# Define the three asymptotic curve parameters a,b,c using the data for the current chunk of data.
			# Compare to previous chunk's aVal, if current chunk is less then use previous chunk's aVal
			if(exists('aValOld')){
				aVal = max(hallData[st:end],na.rm=TRUE)+1 # add 1 count to avoid Inf output
				if (aValOld > aVal){
					aVal = aValOld  # if the current aVal is lower than the previous aValOld, keep using the aValOld
				} else if (aValOld <= aVal){
					aValOld = aVal # Update aValOld to use this new larger aVal from the current chunk of data
				}
			} else {
				# If aValOld doesn't exist yet, find the aVal value and also set aValOld to equal aVal'
				# aVal=value at maximum opening
				aVal = max(hallData[st:end],na.rm=TRUE)+1 # add 1 count to avoid Inf output
				aValOld = aVal  # store the max value for the next time through this loop
			}
			# bVal=the difference between the highest value (aVal) and the lowest (closed)
			# value in this chunk
			bVal = aVal - 
					min(hallData[st:end],na.rm=TRUE)
			# Calculating c requires the linear model object supplied as cRegress, 
			# which will use the log-transformed 'b' value to estimate the 'c' parameter
			cVal = predict(cRegress, newdata = list(b = bVal))
			
			# Use DistFromCounts function to estimate valve gape distance using the input hall
			# effect count data at each time point and the parameters a,b,c
			temp2 = DistFromCounts(hallData[st:end], 
					a = aVal,
					b = bVal,
					c = cVal)
			
			# Replace any fully-open Inf values with the largest good max distance
			temp2[is.infinite(temp2)] = max(temp2[is.finite(temp2)])
			# Replace any negative distance values with the 0 distance value
			temp2[which(temp2 < 0)] = 0
			# Convert to percentage by dividing all distances by the maximum value
			# outputData[st:end] = (temp2 / max(temp2,na.rm=TRUE)) * 100  # original version 0-100%
			
			orderedtemp2 = temp2[order(temp2)] # order all the distance values from small to large
			
			thresholdindex =  floor(length(temp2) * myPercentThreshold) # find row index of the percentile
			thresholdvalue  = orderedtemp2[thresholdindex] # grab the value that is the percentile limit
			
			if (!exists('oldthresholdvalue')) {
				oldthresholdvalue = thresholdvalue
			} else if (exists('oldthresholdvalue')) {
				if (oldthresholdvalue > thresholdvalue) {
					# The previous chunk's X-percentile value was larger than this chunk's, so use the old x-percentile value
					thresholdvalue = oldthresholdvalue
				} else if (thresholdvalue >= oldthresholdvalue){
					# If this new chunk's x-percentile value is larger than the previous chunk's, update old x-percentile value
					oldthresholdvalue = thresholdvalue
				}
			}
			temp3 = (temp2 / thresholdvalue ) * 100 # divide all values by the percentile threshold value
			temp3[which(temp3 > 100)] = 100 # filter any percentages > 100 to just be 100
			outputData[st:end] = temp3
			
			
			# Round the values off to a reasonable precision
			outputData[st:end] = round(outputData[st:end],1)
			
		}
	}	else if (nrow(mygaps) == 0){
		# None of the chunks was long enough to use, so we have no percentage data to return
		outputData = vector(mode = 'numeric', length = 0L)
	}
	
	outputData	# return the output data vector
}	# end of calcPercentGapeNLME function



###{r cRegress_function}
#cRegressEstimate helps calibrate sensors using CalibFileName.csv file 
################################################################################
################################################################################
# Function cRegressEstimate
# Currently this represents the best data showing how the relationship between
# magnet signal and distance is repeatable and consistent among different
# magnets (all from the same batch/part number). As such, a non-linear curve
# can be fit to the various calibration runs to generate a set of 
# coefficients a,b,c for an asymptotic curve. Then the values from a field 
# mussel can be analyzed to determine the asymptotic maximum count value
# (widest opening) and minimum count value (fully closed), which give us the
# parameters for coefficients a and b in the curve fit. Because the curvature
# coefficient 'c' of the curves is very linear with respect to the coefficient
# 'b' (range of readings), we can plug in the max vs. min range for a particular
# mussel and use that to predict the appropriate curve coefficient 'c' to use
# for that mussel. 

#' A function to generate a set of hall effect calibration data and estimate
#' the relationship between parameters 'b' and 'c', for later use in the 
#' function calcPercentGapeNLME(). 
#' @param SN A text string naming the board serial number, in the form 'SN21'
#' @param Channel numeric value naming the Hall effect sensor channel to be 
#' calibrated (expected values 0-15). 
#' @param gapeCalibFile A text path to the file that holds Hall effect
#' calibration data
#' @return cRegress A linear model object representing the best fit regression
#'  of parameter 'c' on the log-transformed parameter 'b'. 

cRegressEstimate <- function(SN, Channel, gapeCalibFile){
	require(nlme)
	# Create an identifier for the board/channel combination 'SN21Ch01'
	if (Channel < 10){
		newChannel = paste0('0',as.character(Channel))
	} else {
		newChannel = as.character(Channel)
	}
	currentID = paste0(SN,'Ch',newChannel)
	
	# Get list of calibration data
	gapeCalibs = read.csv(gapeCalibFile)
	
	gapeCalibs$Serial = factor(gapeCalibs$Serial)
	gapeCalibs$Trial = factor(gapeCalibs$Trial)
	# Create a unique identifier for each board/channel combination,
	# ensuring that the Channel number is always 2 digits
	gapeCalibs$ID = factor(paste0(gapeCalibs$Serial,'Ch',
					ifelse(gapeCalibs$HallChannel<10,
							paste0('0',gapeCalibs$HallChannel),
							gapeCalibs$HallChannel)))
	
# For any instance where magnet was set up so that values climbed above 512
# when close to the sensor, reverse those values so they are all less than 
# 512. 
	for (i in 1:length(levels(gapeCalibs$ID))){
		if (max(gapeCalibs$Reading[gapeCalibs$ID == 
								levels(gapeCalibs$ID)[i]]) > 515){
			gapeCalibs$Reading[gapeCalibs$ID == 
							levels(gapeCalibs$ID)[i]] = 1023 - 
					gapeCalibs$Reading[gapeCalibs$ID == 
									levels(gapeCalibs$ID)[i]]
		}
	}	
	# Begin generating the asymptotic curve fit parameters for the magnetic
# hall effect sensor data. 
	gC2 = groupedData(Reading~Distance.mm|ID/Trial,data=gapeCalibs)
	
	
# Fit the asymptotic model and generate a set of coefficient values
# This will adjust each set of a,b,c coefs based on a random effect of
# magID and Trial. 
	
	# Fit separate curves to each group (trial) in the grouped data frame
	mod3 = nlsList(Reading~a-(b*exp(-c*Distance.mm)), 
			data = subset(gC2,subset = ID == currentID), 
			start = c(a = 460, b = 60, c = 0.3),
			control = list(minFactor = 1e-9, msMaxIter = 200, maxiter = 200))
# Extract the a,b,c coefficients and put them in a data frame
	abcvals = data.frame(a = coef(mod3)[,1], b = coef(mod3)[,2],
			c = coef(mod3)[,3])
	
# The fit between 'b' and 'c' can be linearized with a log transform of 'b'
	cRegress = lm(c~log(b), data = abcvals)  
	
#	plot(c~`log(b)`, data = cRegress$model) # Plot the original data
#	abline(cRegress) # fitted regression line
	
	# Return cRegress as the output
	cRegress
}  # end of cRegressEstimate function.
################################################################################


```


```{r loadFileFunctions, echo=showcode}
# TODO: Adapt these functions to work with the lab mussel metadata files

########################################
# loadFieldMetaData function
#' Load a csv data file containing deployment metadata and maintenance data
#' 
#' The deployment metadata file should contain entries for each oyster and 
#' its associated hall effect sensor channel for a given period of time denoted 
#' by the timestamps in 2 columns titled StartIncludeUTC and EndIncludeUTC, which
#' are assumed to have Excel-formatted date and time stamps in the UTC time zone
#' marking the start and end of each known-good deployment period (thus ignoring
#' time periods when the sensors were pulled from the mooring for maintenance or
#' other interruptions). 
#' 
#' @param filename The path and filename of the metadata csv file
#' @param timezone The timezone of the timestamp data in the metadata file. 
#' Default = UTC.
#' @return A data frame containing the same original columns as the metadata 
#' file, but with timestamps formatted as POSIXct values in the appropriate
#' time zone. 

loadFieldMetaData <- function(filename, timezone = 'UTC'){
	metadata = read.csv(file= filename)
	metadata$StartIncludeUTC = as.POSIXct(metadata$StartIncludeUTC,
			format = '%m/%d/%Y %H:%M',tz = 'UTC')
	metadata$EndIncludeUTC = as.POSIXct(metadata$EndIncludeUTC,
			format = '%m/%d/%Y %H:%M',tz = 'UTC')
	metadata$timeofdeath = as.POSIXct(metadata$timeofdeath,
			format = '%m/%d/%Y %H:%M',tz = 'UTC')
	metadata # return data frame
}

loadFieldMaintData <- function(filename,timezone = 'UTC'){
	maintdata = read.csv(file=filename)
	maintdata$StartMaintUTC = as.POSIXct(maintdata$StartMaintUTC,
			format = '%m/%d/%Y %H:%M',tz = 'UTC')
	maintdata$EndMaintUTC = as.POSIXct(maintdata$EndMaintUTC,
			format = '%m/%d/%Y %H:%M',tz = 'UTC')
	maintdata # return data frame
}

################################################################
# ConcatGapeFiles
# This function deals with the GAPE data files that have an 
# unnecessary comma at the end of each row of data, which messes with the 
# read.csv import function

#' Concatenate multiple daily gape files into one data frame
#' 
#' @param filenames A vector of filenames (including path) to be concatenated
#' @param myTimeZone A text representation of the timezone that the datalogger 
#' clock was set to (i.e. 'UTC' or 'PST8PDT' or 'etc/GMT+8')
#' @param startDate Used to trim start of data frame
#' @param endDate Used to trim end of data frame
#' @param verbose Turn on the progress bar if set to TRUE
#' @return A data frame consisting of the input files concatenated and sorted chronologically. 

# ConcatGapeFiles <- function(filenames, myTimeZone = 'UTC',verbose=TRUE){
ConcatGapeFiles <- function(filenames, myTimeZone = 'UTC', code = '', 
		startDate = NULL, endDate = NULL, verbose=TRUE){
	
	if (exists('dat')) { rm(dat)} # clear dat from memory
	if (verbose){
		pb = txtProgressBar(min=0,max = length(filenames), style = 3)
	}
# Open the raw data files and concatenate them.
	for (f in 1:length(filenames)){
		if(verbose) setTxtProgressBar(pb,f)
		con = file(filenames[f])
		line1 = scan(con, what='character', nlines = 1, sep = ',', quiet = TRUE)
		line2 = scan(con, what = 'character', skip = 1, nlines = 1, sep = ',', quiet = TRUE)
		close(con)
		if (length(line2) > 0){
			if (length(line2) > length(line1)){
				# There is an extra column in the data section due to a trailing comma in the data rows so we need to handle the column names correctly on import
				dattemp = read.csv(filenames[f], skip = 1, header = FALSE,
						col.names = c(line1,'Extra'))
				# Remove the extra column at the end
				dattemp = dattemp[,-(ncol(dattemp))]
			} else {
				# number of headers is same as number of data columns, so just import as normal
				dattemp = read.csv(filenames[f])
			}
		
		
			###########################
			# Columns:
			# POSIXt: elapsed seconds since 1970-01-01 00:00:00 (unix epoch) in whatever
			#         timezone the sensor was set to during deployment. Presumably UTC
			# DateTime: human-readable character date and time, in whatever timezone the
			#         sensor was set to during deployment 
			# SN: serial number of the sensor
			# Hall: raw Hall effect gape sensor reading
			# TempC: temperature in Celsius from MAX30101 sensor on the heart rate dongle	
			# Battery.V:  Supply battery voltage
			#########################
			# Convert the DateTime column to a POSIXct object.
			dattemp$DateTime = as.POSIXct(dattemp$DateTime, tz=myTimeZone, format="%Y-%m-%d %H:%M:%S") 
			# Look for any dates that happened before July 2022, these must be mis-entered
			# time values from Jan 2023 that were recorded as Jan 2022
			if (min(dattemp$DateTime) < as.POSIXct('2022-07-01')){
				adjustYearRows = which(dattemp$DateTime < as.POSIXct('2022-07-01'))
				# Add one year's worth of seconds to the DateTime value
				dattemp$DateTime[adjustYearRows] = dattemp$DateTime[adjustYearRows] + (60*60*24*365)
			}
		
		
			# Concatenate the data files. 
			if (exists('dat')){
				dat = rbind(dat,dattemp)
			} else {
				dat = dattemp
			}
		}
	}
	
	# Add in a column with the oyster code that was passed as an argument to ConcatGapeFile
#	dat$Code = code 
	
	# Reorder the concatenated data frame by the DateTime values in case the files were not fed in in chronological order
	dat = dat[order(dat$DateTime),]
# If the user specified startDate and/or endDate, use those to trim the dataset
# to remove time points before the startDate or after the endDate
	if (!is.null(startDate)){
		if (!is.na(startDate)){
			dat = dat[dat$DateTime>=startDate, ]	
		}		
	}
	
	if (!is.null(endDate)){
		if (!is.na(endDate)){
			dat = dat[dat$DateTime<=endDate, ]
		}
	} 		

	if(verbose) close(pb)
	dat  # return this data frame
}

################################################################
#' Concatenate heart data from raw data files
#' 
#' @param filenames A list of filenames from the datalogger (full path)
#' @param myTimeZone A text representation of the timezone that the datalogger 
#' clock was set to (i.e. 'UTC' or 'PST8PDT' or 'etc/GMT+8')
#' @param code Not used here, could be used if an animal switches serial numbers
#' @param startDate Used to trim start of data frame
#' @param endDate Used to trim end of data frame
#' @param verbose Set to TRUE if you want a text progress bar to show
#' @param sampleLength The number of samples in a sampling bout. Common values 
#' are 400 or 240 (50 seconds or 30 seconds at 8Hz interval of BivalveBit)

ConcatHeartFiles <- function(filenames, myTimeZone = 'UTC', code = '', 
		startDate = NULL, endDate = NULL, verbose=TRUE, sampleLength){
	
	if (exists('Heart')) {rm(Heart)} # clear old version of Heart
	if (verbose){
		pb = txtProgressBar(min=0,max = length(filenames), style = 3)
	}
	
	# pull HR data and put in Heart
	for (i in 1:length(filenames)){
		if(verbose) setTxtProgressBar(pb,i)
		if(exists('temp')) {rm(temp)} # clear old version of temp
		
		hdr  = scan(filenames[i], nlines = 1, what = character(),sep = ',', 
				quiet = TRUE)
		# Use the try() function to handle cases where the input file
		# might be empty
		try(temp <- read.table(filenames[i],skip = 1, header = FALSE, 
						sep = ',', na.strings=c("","NA")), silent=TRUE)
		if (!exists('temp')){
			next  # If the data frame temp wasn't created, skip the rest 
			# of this iteration of the loop
		}
		
		colnames(temp) = hdr
		temp$DateTime  = as.POSIXct(temp$DateTime, tz = 'UTC', 
				format="%Y-%m-%d %H:%M:%S")
		
		# The original data files only had a timestamp every 400 rows
		# at the start and end of a 400-sample period. It might be useful
		# to fill in the intervening time values
		# To preserve this information in csv files, you need to have
		# set R's options to show 3 digits on time values: 
		# options(digits.secs = 3)
		################################################################
		# Get all rows that have a non-NA timestamp value in them
		goodvals = which(!is.na(temp$DateTime))
		# Get the subset where the last digit of the row number is a 1
		# rather than a 0, since these are the rows at the start of 
		# each 400-sample chunk (row 1, row 401, row 801 etc.)
		startRows = goodvals[which((goodvals %% 10)== 1)]
		# Go through the 400 values and add an incrementing milliseconds
		# value onto the timestamp.
		# Add 0.125 seconds to each reading (assumes a 8Hz sample rate). Very 
		# last reading is the last millisecond in 50 seconds. (49.875seconds) if
		# 400 sample is the sampleLength
		for (k in 1:length(startRows)){
			nextStart = startRows[k]
			# Generate a set of sampleLength timestamps incrementing by 0.125sec
			tvals = seq(temp$DateTime[nextStart], by = 0.125, 
					length.out = sampleLength)
			temp$DateTime[nextStart : (nextStart+sampleLength-1)] = tvals
		}
		
		# If the user specified startDate and/or endDate, use those to trim the dataset
		# to remove time points before the startDate or after the endDate
		if (!is.null(startDate)){
			if (!is.na(startDate)){
				temp = temp[temp$DateTime>=startDate, ]	
			}		
		}
		
		if (!is.null(endDate)){
			if (!is.na(endDate)){
				temp = temp[temp$DateTime<=endDate, ]
			}
		} 	
		
		if(nrow(temp)==0){
			next
		}
		
		if(!exists('Heart')){
			Heart = temp
		} else if (exists('Heart')){
			Heart = rbind(Heart,temp)
		}
	} # end of (i in 1:length(filenames)) loop

	close(pb)
	Heart # return the Heart data frame
}  # End of function

##############################################################
#' Excise questionable rows of data from Hall effect sensor data
#' 
#' New idea: take a halldata data frame, which may have a few disjointed periods
#' of sampling data (from different serial numbers), and map those data onto
#' a single continuous time series data frame. Then go through the low tide metadata
#' file and identify all the rows that apply to this particular oyster (based on 
#' its Code value, which gives site and treatment info), and then use the low tide
#' servicing start and end times to further NA any rows in the giant time series that
#' were during low tide service periods. At the end of that, then take the giant
#' time series and interpolate any small gaps in data, and return the big time
#' series as a data frame that will later get saved to a csv file. 
#' 
#' @param halldata A dataframe containing timestamps and hall effect data from
#' multiple sensors.
#' @param metadf A data frame of start and end times to use in the data set,
#' formatted as POSIXct time stamps in the same time zone as values in the 
#' timestamps argument. The data frame should also have a column with an OysterID
#' and a hall effect sensor channel for that oyster for each time period laid
#' out in the data frame.
#' @param maxGapFill The maximum number of missing values in the input time 
#' series to impute via linear interpolation. Gaps longer than this will remain 
#' as NA values. 
#' @param maintenance A data frame of start and end times of maintenance of 
#' battery changes in the field. Start time=start of maintenance. End time=
#' end of maintenance. Separated by site and treatment. 
#' @param timeofdeath A timestamp of approximate death of oyster in field. Column
#' within metadf. 
#' @return A data frame with columns
#' added on for each oyster with available hall effect sensor data. Missing 
#' periods of hall effect values will be denoted with NA values.  

#halldata = gapedat
#metadf = metafile
exciseHall = function(halldata, metadf, maintenance, timeofdeath, maxGapFill = 12) {
	# Get the list of unique OysterID values from excised
	# oysters = halldata$SN[1]
	#oysters = oysters[order(oysters)] # reorder if needed 
	
	# Generate output data frame based on initial deployment time and final 
	# raw data time
	time1 = min(metadf$StartIncludeUTC, na.rm=T)
	time2 = max(halldata$DateTime, na.rm=T)
	totaltimeseries = seq(time1,time2,by=60) #every 60 seconds
	# Create vector to hold hall effect data for this oyster
	temphall = matrix(data = NA, nrow = length(totaltimeseries), ncol = 5)
	colnames(temphall) = c('Hall', 'Temp.C','Battery.V','SN', 'Code')
	# Convert outputdf to a zoo object for easier time handling
	tempoutput = zoo(temphall,order.by = totaltimeseries)
	
	# Remove any possible duplicate timestamps, usually caused by the sensor
	# briefly resetting itself (losing power) within the same minute
	dupeRows = (which(diff(halldata$POSIXt) < 1) + 1)
	if (length(dupeRows)!=0){
		halldata = halldata[-dupeRows, ]
	}
	
	
	# Convert the input halldata into a zoo object also
	temp = zoo(halldata[,c('Hall', 'Temp.C','Battery.V','SN', 'Code')], 
			order.by = halldata[,"DateTime"], frequency = 2)
	
	# Insert the temp data into the zoo object tempoutput
	# The syntax is fun here: On the left side of the equals sign,
	# you want a set of indices in tempoutput where the time value
	# from tempoutput is found %in% the time values of temp. And on the
	# right side of the equals sign, you want to get hall effect values
	# from temp where the time value in temp is found %in% the time
	# values of tempoutput (a much bigger set)
	tempoutput[index(tempoutput) %in% index(temp), ] = 
			temp[index(temp) %in% index(tempoutput),]
	
	#######################################################
	# At this point tempoutput should be a long zoo object with timestamnps
	# running from the date of the first deployment to the last available date
	# of this particular oyster, with NA's in rows where hall/temperature data
	# weren't available
	#######################################################
	# Go to the low tide servicing metadata file and get the relevant
	# start and end times for low tide service periods for the site/treatment
	# that this oyster is at (contained in the Code column)
	
	sitecode = substr(halldata$Code[1],1,1)
	treatmentcode=substr(halldata$Code[1],3,3)
	# Go to low tide service metafile and find all rows that match
	# this sitecode (and treatment code) and get the list of those row numbers
	# Then cycle through those row numbers, in each case grabbing the start time
	# and end time of the low tide servicing. Then go to tempoutput and find
	# any rows that are > start time and < end time, and convert NAs. 
	services = which( (maintenance$sitecode == sitecode) & (maintenance$treatmentcode == treatmentcode) )
	for (k in (services) ){
		# Find the matching time stamps and write NAs in between them
		startTime = maintenance$StartMaintUTC[k]
		endTime = maintenance$EndMaintUTC[k]
		# This may not work with tempoutput in the zoo format, but the point is to 
		# NA those lines that are within the startTime and endTime boundaries
		tempoutput[which((index(tempoutput) > startTime) & (index(tempoutput) < endTime)),] = NA
		#tempoutput[which((index(temp) > startTime) & (index(temp) < endTime)),] = NA
	}
	
	
	# Convert tempoutput back to a data frame and stick it onto an output
	# dataframe
	outputdf = data.frame(DateTime = index(tempoutput), tempoutput)
	#outputdf = data.frame(DateTime = index(temp), temp)
	# Remove hall data for after an oyster has died
	
	if (!is.null(timeofdeath)){
		if(!is.na(timeofdeath)){
			outputdf = outputdf[outputdf$DateTime<timeofdeath, ]
		}
	}
	
	outputdf$Hall = as.numeric(outputdf$Hall)
	outputdf$Temp.C = as.numeric(outputdf$Temp.C)
	outputdf$Battery.V = as.numeric(outputdf$Battery.V)
	
	
	outputdf  # return outputdf	
	
} # End of exciseHall function
```

# Import the data

## Import metadata

```{r importMetadata}
# readxl package is part of the tidyverse packages

# Import the calibration sheet
gapeCalibs = readxl::read_excel(paste0(localpath, metafilename), 
		sheet = 'GapeCalibrations')
# Convert MusselID to factor, and remove the underscore in the new column name
gapeCalibs$MusselID = factor(gapeCalibs$Mussel_ID)
# Convert from tibble garbage to a regular data frame for regular people
gapeCalibs = data.frame(gapeCalibs)

# Import the mussel metadata sheet
musselMetaData = readxl::read_excel(paste0(localpath, metafilename), 
		sheet = 'MusselMetadata')
musselMetaData$MusselID = factor(musselMetaData$MusselID)

# Import the sheet with information on start/stop times for trials
trialMetaData = readxl::read_excel(paste0(localpath, metafilename), 
		sheet = 'TrialMetadata')
trialMetaData$MusselID = factor(trialMetaData$MusselID)
trialMetaData$SerialNumber = factor(trialMetaData$SerialNumber)
trialMetaData$SalinityTreatment = factor(trialMetaData$SalinityTreatment)
# The metadata times were recorded in local time (Pacific Daylight Time), but 
# the read_excel function assumes they're UTC time zone. Use this to swap them
# to Pacific Daylight Time zone without actually changing the time value
trialMetaData$AcclimationStartTimePDT = 
		lubridate::force_tz(trialMetaData$AcclimationStartTime, tzone = 'PST8PDT')
trialMetaData$TrialStartTimePDT = 
		lubridate::force_tz(trialMetaData$TrialStartTime, tzone = 'PST8PDT')
trialMetaData$TrialStopTimePDT = 
		lubridate::force_tz(trialMetaData$TrialStopTime, tzone = 'PST8PDT')


```

### Process the calibration data

Mussel gape was calibrated using rods of known diameter stuck into the gaping
shells at the location of the hall effect sensor/magnet combination, and then
recording the raw Hall effect reading while the mussel was clamped down on the
rod. 

```{r parseGapeCalibData, echo = showcode}
# Process the gape calibration data for each mussel. They are in long format
# so you need to find all the rows with data for a given mussel, fit a 
# regression to gauge diameter vs Hall reading, and retain the slope/intercept
# information for that mussel

# Get a list of mussel IDs
ids = unique(gapeCalibs$MusselID)


for (i in 1:length(ids)){
	# Subset out all rows for this ID
	temp = gapeCalibs[which(gapeCalibs$MusselID == ids[i]),]
	
	# If there are more than 2 gape measurements, attempt to fit the regression
	# and otherwise skip this mussel (and perhaps print a warning)
	if (nrow(temp) > 2){
		# Fit a regression line through these data points
		mymod = lm(GaugeDiameter.mm ~ HallReading, data = temp)
		if (i == 1){
			# Save the intercept and slope coefficients in a list
			CalibFits = data.frame(MusselID = as.character(ids[i]),
					Intercept = mymod$coefficients["(Intercept)"], 
					Slope = mymod$coefficients[2])	
		} else if (i > 1) {
			CalibFits = rbind(CalibFits, 
					data.frame(MusselID = as.character(ids[i]),
							Intercept = mymod$coefficients["(Intercept)"], 
							Slope = mymod$coefficients[2]))
		}
	} else {
		warning('Not enough calibration data to process ', ids[i])
	}
}

# Add the datalogger serial number to each mussel (this will need future work
# if a mussel switches between datalogger serial numbers during use)
CalibFits$SerialNumber = NA
for (i in 1:nrow(CalibFits)){
	whichSN = as.character(trialMetaData$SerialNumber[which(as.character(trialMetaData$MusselID) == 
							as.character(CalibFits$MusselID[i]))])
	if (length(whichSN) > 0){
		CalibFits$SerialNumber[i] = whichSN
	}
}

# With the output CalibFits, you should be able to take any raw Hall reading
# for a mussel (matching MusselID) and estimate a gape opening using the
# regression slope and intercept stored in CalibFits

```



## Import and concatenate gape files

The individual daily valve gape files need to be imported for each 
datalogger/mussel and concatenated into a single long file for each animal. The
resulting files are written as CSV files that can be reopened later in the 
script for further processing. The datalogger clocks should all have been set
to UTC time zone, so this script assumes that when converting dates and times 
in the data files.

```{r concatGape, echo=showcode, eval = evalAll}
# In practice you should only need to run this chunk 1 time to generate the 
# concatenated gape data files for each datalogger (serial number). If new data
# are later collected for that same datalogger/mussel, you'd need to run this
# again to generate the new, longer concatenated file. Otherwise, you should
# primarily be working from the concatenated data files in later chunks

# Get a list of datalogger serial numbers to process
SNnames = dir(path=paste0(localpath,trialpath),recursive=FALSE)
# Should return folder names like "SN103", "SN173"

for (sn in SNnames){
	# sn will hold the text representation of one of the folders (i.e. 'SN103')
	# Get a list of the GAPE files in the appropriate data folder
	fnames = dir(path=paste0(localpath,trialpath,sn), pattern="*GAPE.csv",
			full.names=TRUE)
	
	tempData = ConcatGapeFiles(filenames=fnames,myTimeZone = 'UTC',verbose=TRUE)
	# Take the data in tempData and put it in a data frame named based on 
	# the serial number being processed
#	assign(sn,tempData)
	# Write the concatenated data to a csv file
	write.csv(tempData, file = paste0(localpath,outputgapedir,sn,'_gape.csv'),
			row.names=FALSE)
}


```

## Import and concatenate heart data

The individual daily heart data files for each datalogger/mussel need to be 
imported, parsed, and concatenated into one long output file per datalogger.
The script assumes that in this case the sampling rate for heart data was 8
samples per second and that a set of 400 samples was collected during each
sampling interval. 400 samples at 8Hz = 50 seconds of samples. In this lab trial
loggers were set to collect a new set of samples every 2 minutes (every even
minute). The concatenated data are written as CSV files that can be reopened
later in the script for further processing rather than having to do this long
ingestion process every time. The datalogger clocks should all have been set to
UTC time zone, so this script assumes that when converting dates and times in
the data files.

```{r concatHeart,echo=showcode, eval = evalAll}
# In practice you should only need to run this chunk 1 time to generate the 
# concatenated gape data files for each datalogger (serial number). If new data
# are later collected for that same datalogger/mussel, you'd need to run this
# again to generate the new, longer concatenated file. Otherwise, you should
# primarily be working from the concatenated data files in later chunks

# This chunk will take 10's of minutes to run if you have lots of files
# to process (much longer than the gape processing)

#########################################
# Get a list of datalogger serial numbers to process
SNnames = dir(path=paste0(localpath,trialpath),recursive=FALSE)
# Should return folder names like "SN103", "SN173"

for (sn in SNnames){
	# sn will hold the text representation of one of the folders (i.e. 'SN103')
	# Get a list of the IR (heart sensor) files in the appropriate data folder
	hnames = dir(path=paste0(localpath,trialpath,sn), pattern="*IR.csv",
			full.names=TRUE)
	
	tempData = ConcatHeartFiles(filenames = hnames, myTimeZone = 'UTC', 
			verbose = TRUE, sampleLength = 400)
	# Take the data in tempData and put it in a data frame named based on 
	# the serial number being processed
#	assign(sn,tempData)
	
	# Write the concatenated data to a csv file
	write.csv(tempData, file = paste0(localpath,outputheartdir,sn,'_heart.csv'),
			row.names=FALSE)
}

```


## Import salinity/temperature/DO data from trials

TODO: Add code to import associated water quality parameters during the trials

# Process gape data

The concatenated gape files will contain gape data from before, during, and
after the trials, so we'll want to subset out the relevant chunks of time
for the mussels involved in a particular trial. The time data should be stored
in the `trialMetaData` data frame. 





```{r loadConcatGapeDataTrial1, echo=showcode}
# Reimport the concatenated gape data for relevant mussels

# Get the list of MusselID's / SerialNumbers for each mussel involved in Trial1

myTrial = 1

snsControl = as.character(
		trialMetaData$SerialNumber[which(
						trialMetaData$SalinityTreatment == 'control' & 
						trialMetaData$TrialNumber == myTrial)])

snsLowSal = as.character(
		trialMetaData$SerialNumber[which(
						trialMetaData$SalinityTreatment == '3psu' & 
								trialMetaData$TrialNumber == myTrial)])


for (i in 1:length(snsControl)){
	temp = read.csv(file = paste0(localpath,outputgapedir,
					snsControl[i],'_gape.csv'))
	# Get start and stop time from trialMetaData 
	startTime = trialMetaData$TrialStartTimePDT[which(trialMetaData$SerialNumber ==
							snsControl[i])]
	stopTime =  trialMetaData$TrialStopTimePDT[which(
					trialMetaData$SerialNumber == snsControl[i])]

	# Convert DateTime into POSIXct format and shift to PDT time zone
	temp$DateTime = as.POSIXct(temp$DateTime, tz = 'UTC')
	attr(temp$DateTime, 'tzone') = 'PST8PDT'
	# Trim the start and stop times
	temp = temp[which(temp$DateTime >= startTime),]
	temp = temp[which(temp$DateTime <= stopTime),]
	
	assign(paste0(snsControl[i],'gape'), value = temp)
}


for (i in 1:length(snsLowSal)){
	temp = read.csv(file = paste0(localpath,outputgapedir,
					snsLowSal[i],'_gape.csv'))
	# Get start and stop time from trialMetaData 
	startTime = trialMetaData$TrialStartTimePDT[which(trialMetaData$SerialNumber ==
							snsLowSal[i])]
	stopTime =  trialMetaData$TrialStopTimePDT[which(
					trialMetaData$SerialNumber == snsLowSal[i])]
	
	# Convert DateTime into POSIXct format and shift to PDT time zone
	temp$DateTime = as.POSIXct(temp$DateTime, tz = 'UTC')
	attr(temp$DateTime, 'tzone') = 'PST8PDT'
	# Trim the start and stop times
	temp = temp[which(temp$DateTime >= startTime),]
	temp = temp[which(temp$DateTime <= stopTime),]
	
	assign(paste0(snsLowSal[i],'gape'), value = temp)
}


quickplot = function(GapeData){
	plot(GapeData$DateTime, y = GapeData$Hall, type = 'l')
}

# Controls, Trial 1
# quickplot(SN173gape)  # data look okay
# quickplot(SN196gape)  # data look okay
# quickplot(SN224gape)  # data look okay

# Low salinity Trial 1
#quickplot(SN189gape)  # data look okay - low for middle of trial
#quickplot(SN218gape)  # data look okay - low for middle of trial
# quickplot(SN103gape) # no data past 8/24/2023, don't use
#quickplot(SN179gape)  # data look okay - low for middle of trial

# Apply the calibration data to estimate gape opening in mm for each mussel

SN173gape = HalltoMM(SN173gape,CalibFits)
SN196gape = HalltoMM(SN196gape, CalibFits)
SN224gape = HalltoMM(SN224gape, CalibFits)
SN189gape = HalltoMM(SN189gape, CalibFits)
SN218gape = HalltoMM(SN218gape, CalibFits)
SN179gape = HalltoMM(SN179gape, CalibFits)

SN173gape$gape.pc = calcPercentGape(SN173gape$gape.mm)
SN196gape$gape.pc = calcPercentGape(SN196gape$gape.mm)
SN224gape$gape.pc = calcPercentGape(SN224gape$gape.mm)
SN189gape$gape.pc = calcPercentGape(SN189gape$gape.mm)
SN218gape$gape.pc = calcPercentGape(SN218gape$gape.mm)
SN179gape$gape.pc = calcPercentGape(SN179gape$gape.mm)

```
From the 1st trial, there appears to be good gape data from control mussels
SN173, SN196, and SN224. For the low salinity mussels, SN189, SN218, and SN179
appear to be usable. 



```{r loadConcatGapeDataTrial2, echo=showcode}
# Process the 2nd trial's gape data
myTrial = 2

snsControl = as.character(
		trialMetaData$SerialNumber[which(
						trialMetaData$SalinityTreatment == 'control' & 
								trialMetaData$TrialNumber == myTrial)])

snsLowSal = as.character(
		trialMetaData$SerialNumber[which(
						trialMetaData$SalinityTreatment == '3psu' & 
								trialMetaData$TrialNumber == myTrial)])


for (i in 1:length(snsControl)){
	temp = read.csv(file = paste0(localpath,outputgapedir,
					snsControl[i],'_gape.csv'))
	# Get start and stop time from trialMetaData 
	startTime = trialMetaData$TrialStartTimePDT[which(trialMetaData$SerialNumber ==
							snsControl[i])]
	stopTime =  trialMetaData$TrialStopTimePDT[which(
					trialMetaData$SerialNumber == snsControl[i])]
	
	# Convert DateTime into POSIXct format and shift to PDT time zone
	temp$DateTime = as.POSIXct(temp$DateTime, tz = 'UTC')
	attr(temp$DateTime, 'tzone') = 'PST8PDT'
	# Trim the start and stop times
	temp = temp[which(temp$DateTime >= startTime),]
	temp = temp[which(temp$DateTime <= stopTime),]
	
	assign(paste0(snsControl[i],'gape'), value = temp)
}


for (i in 1:length(snsLowSal)){
	temp = read.csv(file = paste0(localpath,outputgapedir,
					snsLowSal[i],'_gape.csv'))
	# Get start and stop time from trialMetaData 
	startTime = trialMetaData$TrialStartTimePDT[which(trialMetaData$SerialNumber ==
							snsLowSal[i])]
	stopTime =  trialMetaData$TrialStopTimePDT[which(
					trialMetaData$SerialNumber == snsLowSal[i])]
	
	# Convert DateTime into POSIXct format and shift to PDT time zone
	temp$DateTime = as.POSIXct(temp$DateTime, tz = 'UTC')
	attr(temp$DateTime, 'tzone') = 'PST8PDT'
	# Trim the start and stop times
	temp = temp[which(temp$DateTime >= startTime),]
	temp = temp[which(temp$DateTime <= stopTime),]
	
	assign(paste0(snsLowSal[i],'gape'), value = temp)
}


# Trial 2 control
#quickplot(SN214gape) # data look okay
#quickplot(SN242gape) # stayed closed entire trial
#quickplot(SN217gape) # Not collecting data during trial
#quickplot(SN151gape) # no data, malfunctioning sensor reading 1023 only

# Trial2 # low salinity treatment
#quickplot(SN243gape)  # fully closed entire trial until very end - probably okay
#quickplot(SN198gape)  # briefly open at start and very end of trial - probaly okay
#quickplot(SN245gape)  # malfunctioning sensor reads 1023 only
#quickplot(SN226gape)  # malfunctioning sensor during trial drifts > 500



SN214gape = HalltoMM(SN214gape, CalibFits)
SN242gape = HalltoMM(SN242gape, CalibFits)
SN243gape = HalltoMM(SN243gape, CalibFits)
#SN198gape = HalltoMM(SN198gape, CalibFits) # need calibration data

SN214gape$gape.pc = calcPercentGape(SN214gape$gape.mm)
SN242gape$gape.pc = calcPercentGape(SN242gape$gape.mm)
SN243gape$gape.pc = calcPercentGape(SN243gape$gape.mm)
#SN198gape$gape.pc = calcPercentGape(SN198gape$gape.mm) # need calibration data

```

For the 2nd trial, it appears that there are only usable gape data for two of
the control mussels (SN214 and SN242, but 242 was closed the entire time), and
gape data from low-salinity mussels SN243 and SN198 (but SN198 lacks 
calibration data currently). 


# Process heart data

#getBPM function
```{r getBPMfunction,echo=showcode}
# A function to convert raw heartbeat timeseries into estimates of heart rate
# (beats per minute). This function uses the filtfilt routine from the signal
# library and a butterworth filter. If you specify a particular frequency 
# for the lowband (and optionally highband), it will effectively filter
# at half that frequency. For example, if you specify a lowband value of 
# 1/10 (0.1 = 10 Hz), you get a lowpass filter that cuts off signals above around 5Hz.
# If you specify 1/20 (0.05 = 5Hz), the lowpass filter filters anything above
# about 2.5Hz.

#' @param x A data frame with columns DateTime and IR for sensor data
#' @param t1 The starting index of data to be subset from x
#' @param sensor Column name in x declaring the sensor that you want to process
#' @param chunklength Number of samples to process, default = 400 is 50 seconds at 8Hz for BivalveBit loggers
#' @param lowband Frequency for the lower frequency of the bandpass filter, consider values from 1/20 to 1/300
#' @param highband Frequency for the upper frequency of the bandpass filter, consider values from 1/2 to 1/20. Set to NULL to 
#' implement a lowpass filter only
#' @param amplitudeThreshold Minimum peak to peak amplitude of the *filtered* heart signal
#' @param maxBPM Maximum beats per minute estimate, higher values will be flagged in the output
#' @param Fs Sampling interval in seconds (default 0.125 = 8Hz for BivalveBit loggers)
#' @param BPMqualityThreshold A numeric value indicating how close two heart rate estimates must be to be considered in agreement 
#' @param plot Logical value to show a plot or not. Default is FALSE.
#' @param myYlims Set of fixed y-axis limits for plots
#' 
#' 
#' @return A list containing 3 estimates of the beats per minute, along with
#' values indicating how close the 3 estimates agree and whether the signal may
#' be questionable.

getBPM = function(x,t1 = 1, sensor = 'IR',chunklength=400, 
		lowband = 1/100, 
		highband = 1/15, 
		amplitudeThreshold = 60,
		maxBPM = 80,
		Fs = 0.125,
		BPMqualityThreshold = 4,
		plot = FALSE,
		myYlims = NULL)
{
	temp = x[t1:(t1+chunklength-1),]  #Grab the chunk of data
	
	# 
	# Grab a chunk of data that's twice as long as we need
#	temp = x[t1:(t1+(chunklength*2)),]
	# Calculate time difference between each sample (milliseconds), should be 100
#	diffs = diff(temp$startMillis)
	# Identify any gaps in the data where the interval was greater than the sampling interval in milliseconds
#	missedReads = which(diffs > (1000 * Fs))
	
# Check if there are any sampling gaps (missedReads)
#	if (length(missedReads) > 0){
#		# Add on the final row as well
#		missedReads = c(missedReads, (nrow(temp)+1) )
#		# Calculate gap length between any missed reads (and the final read)
#		testgaps = diff(missedReads) 
#		if (missedReads[1] > chunklength){
#			# In this case, just grab the first 300 readings
#			temp = temp[1:chunklength,]
#		} else if (length(which(testgaps>=chunklength)) > 0){
#			# check if any of the testgaps values are >=300 (chunklength)
#			# Get the index in temp that is at the start of the long run
#			tempindx = missedReads[which(testgaps >= chunklength)]+1
#			# If a gap is > 300, grab the sample in that gap
#			temp = temp[tempindx:(tempindx+chunklength-1),]
#		} else if (length( which(testgaps>=chunklength) ) == 0) {
#			# In this case there may be multiple gaps in the time chunk, 
#			# so that there are no good contiguous chunks of 300 readings
#			# Return a data frame with NAs
#			temp[,sensor] = NA 
#			
##			tempindx = missedReads[length(missedReads)]+1
##			temp = temp[tempindx:(tempindx+chunklength-1),]
#			# It's also possible to end up here if you grabbed a chunk that
#			# ran off the end of the 600 samples and returned some NAs. That
#			# will be handled below
#			
#		} 
#	} else if (length(missedReads) == 0) {
#		# Subset down to exactly the chunk length (10Hz sample * 30 secs = 300 samples)
#		temp = temp[1:chunklength,]
#	}
	

	# Add milliseconds onto the timestamps
	temp$DateTimeMS = temp$DateTime
	# create a new set of timestamps that have millisecond values attached
#	myvec=vector(mode="numeric", length=chunklength-1)
#	myvec[1:length(myvec)]=Fs
#	temp$DateTimeMS[2:nrow(temp)] = temp$DateTimeMS[1] + cumsum(myvec)

	
	# Test if there are any NAs in the heart rate values, if there are not
	# then proceed with the filtering and heart rate determination
	if ( length( which( is.na(temp[,sensor]) ) ) == 0){
		
		# Detrend the heartrate readings
		detrendedIR = pracma::detrend(temp[,sensor])	
		# Next look at the detrended values and look for spurious low values
		# For instance, a good heart signal might oscillate between -200 & +300
		# in the detrended data, and a spurious value might suddenly drop to 
		# -6000. 
		# Calculate the standard deviation of the data set, and then find values
		# that are more than 3 SD away from the mean (which should be ~zero in the
		# detrended data)
#		spuriousVals = which(abs(detrendedIR) > (3*sd(detrendedIR)) )

#		if ( length(spuriousVals) > 0) {
#			# Convert to NAs 
#			temp[spuriousVals,sensor] = NA
#			
#			for (i in 1:length(spuriousVals)){
#				indx = spuriousVals[i]  # Get the row index for this spurious value
#				if (indx == 1) {
#					# If the first value is spurious, replace it with a copy 
#					# of the next value
#					temp[indx,sensor] = temp[indx+1,sensor]
#				} else {
#				# Replace the spurious value with the average of the values immediately
#				# before and after the spurious value
##				temp[indx,sensor] = mean(c(temp[indx-1,sensor],temp[indx+1,sensor]))
#				temp[indx,sensor] = mean(temp[(indx-2):(indx+2),sensor], na.rm=TRUE)
#				}
#			}
#		}
#		rm(spuriousVals)

		
		# With the spurious values replaced by interpolated values, re-run the 
		# detrending routine
#		detrendedIR = pracma::detrend(temp[,sensor])
	} else {
		# If there were NAs, just define detrendedIR as NA so that later 
		# operations skip over this chunk of data
		detrendedIR = NA
	}

	

	if (length(which(is.na(detrendedIR))) == 0) {

	# Define a butterworth filter
	# Consider using the bandpass filter rather than just a lowpass filter
	# because of the tendency for the IR heartrate signal to drift up and 
	# down in relation to ambient light, which tends to induce low-frequency
	# shifts that then fool the spectral analysis routines when trying to 
	# identify the dominant frequency
		if (!is.null(highband)) {
			bf = signal::butter(3,W = c(lowband, highband), type = 'pass')	# bandpass filter
		} else if (is.null(highband)){
			bf = butter(3,W = lowband, type = 'low')  #  lowpass filter			
		}

		# Apply the filter to the detrended data chunk
		y = filtfilt(bf, x = detrendedIR)
		# Calculate the spectrum of the filtered data
		myfft2 = spectrum(y, plot = FALSE)
		# Take the peak frequency from the spectrum, divide by sampling
		# rate to convert to cycles per second
		mypeakfreq = myfft2$freq[which.max(myfft2$spec)] / Fs
		# Multiply by 60 seconds to get cycles (beats) per minute
		BPMfft = 60 * mypeakfreq
		# Calculate amplitude of filtered signal to denote weak or noisy signals
		amp = range(y)[2] - range(y)[1]
		# Use function from package 'forecast', returns peak period (not freq)
		forecastPeriod = forecast::findfrequency(y)  
		forecastFreq = 1/forecastPeriod # convert period to frequency
		BPMforecast = forecastFreq * 60 * (1/Fs) # convert frequency to beats per 
		# minute, based on the fact that the sampling period is Fs, and there are
		# 60 seconds in a minute

		# Set a flag for cases where the forecast:findfrequency estimate is 
		# extremely large, which happens when it can't find a clear heart signal
		BPMforecastflag = ifelse(BPMforecast > maxBPM, 'FAIL','OK')
		# Set a flag for cases where the detrended/filtered signal has a very
		# small amplitude, signaling that there may be no good heartbeat signal 
		WeakSignalFlag = ifelse(amp < amplitudeThreshold, 'FAIL','OK')
		# Use pracma package to find peaks. Note that at slower heart rates
		# this function tends to find the sub-peaks (akin to a P or T peak in a
		# human ECG trace) rather than just the main peaks (R peaks on a human). 
		# This happens based on what the bandpass filter lets through. 
		pracPeaks = pracma::findpeaks(x = y, nups = 5, minpeakdistance = 10)
		# Figure out what we need to multiply the number of peaks by in order
		# to get from our less-than-a-minute sampling period to a full minute
		samplingseconds = chunklength * Fs
		minutemultiplier = 60 / samplingseconds  
		# If we sampled for 30 seconds, minutemultiplier should be 2
		# if we sampled for 50 seconds, minutemultiplier should be 1.2
		# Now convert the number of peaks to the number of beats per minute
		BPMprac = nrow(pracPeaks) * minutemultiplier
		
		# Calculate the difference between the estimated heart rates from the 
		# spectrum fft routine and the forecast::findfrequency routine. A 
		# small value indicates good agreement
		BPMagreeQuality.fft.forecast = ceiling(abs(BPMfft - BPMforecast))
		# Calculate difference between the forecast and pracma estimates
		BPMagreeQuality.prac.forecast = ceiling(abs(BPMprac - BPMforecast))
		# Calculate difference between fft and pracma estimates
		BPMagreeQuality.prac.fft = ceiling(abs(BPMprac - BPMfft))
		
		if (plot){
			## Plot the raw detrended signal
			if (!is.null(myYlims)){
				# Use the specified y limits
				plot(temp$DateTimeMS, detrendedIR, type = 'l', 
						main = '', xlab = 'Seconds', ylab = '',
						las = 1, ylim = myYlims)	
			} else if (is.null(myYlims)) {
				# No y limits specified, use plot defaults
				plot(temp$DateTimeMS, detrendedIR, type = 'l', 
						main = '', xlab = 'Seconds', ylab = '',
						las = 1)	
			}
			 
			points(temp$DateTimeMS, detrendedIR, col = 1, pch = 20, cex = 1)
			if (amp > amplitudeThreshold){
				lines(temp$DateTimeMS, y, col = 3, lwd = 2) # add the filtered signal as a green line
			} else if (amp <= amplitudeThreshold) {
				lines(temp$DateTimeMS, y, col = 2, lwd = 2) # add the filtered signal as a red line	
				warning('Weak signal')
			}
			points(temp$DateTimeMS[pracPeaks[,2]], y = pracPeaks[,1], col = 4, pch = 19)
			mtext(side = 2, text = 'Detrended IR signal', line = 2.5, cex = 1)
			mtext(side = 1, line = 3, text = paste(sensor, strftime(temp$DateTime[1], tz="PST8PDT")), cex = 0.8, adj = 1)
			mtext(side = 3, line = 3, text = paste0('pracma bpm: ', BPMprac))
			mtext(side = 3, line = 2, text = paste0('fft bpm: ', BPMfft))
			mtext(side = 3, line = 1, text = paste0('forecast filtered bpm: ', round(BPMforecast,1)))
			if (!is.null(highband)){
				mtext(side = 3, line = 0.01, 
						text = paste0('Bandpass - Lower pass: ', round(lowband,digits =3), ', upper pass: ', round(highband,digits = 3)),
						cex = 0.8)	
			} else if (is.null(highband)) {
				mtext(side = 3, line = 0.01, 
						text = paste0('Lowpass - Lower limit: ', round(lowband,digits =3)),cex = 0.8)	
			}
			
			
		}

		resultsList = list(DateTime = temp$DateTime[1],
				Sensor = sensor,
				BPMfft = round(BPMfft,1),
				BPMforecast = round(BPMforecast,1),
				BPMpeaks = BPMprac,
				BPMagreeQuality.fft.forecast = BPMagreeQuality.fft.forecast,
				BPMagreeQuality.prac.forecast = BPMagreeQuality.prac.forecast,
				BPMagreeQuality.prac.fft = BPMagreeQuality.prac.fft,
				BPMforecastflag = BPMforecastflag,
				WeakSignalFlag = WeakSignalFlag,
				FilteredAmplitude = round(amp,1),
				finalBPM = NA,
				finalfilter = 1/highband,
				QA = FALSE)
		# If there is reasonable agreement between the 3 beats per minute estimates,
		# record the mean of the 3 values as the finalBPM
		if (resultsList$BPMagreeQuality.fft.forecast <= BPMqualityThreshold & 
				resultsList$BPMagreeQuality.prac.forecast <= BPMqualityThreshold &
				resultsList$BPMagreeQuality.prac.fft <= BPMqualityThreshold & 
				resultsList$WeakSignalFlag == 'OK') {
			resultsList$finalBPM = round(mean(resultsList$BPMfft,
							resultsList$BPMforecast,
							resultsList$BPMpeaks), digits = 1)
		}
	} else if (length( which( is.na(temp[,sensor]) ) ) > 0) {
		# Handle the case where there are NAs in the data chunk that prevent
		# the filtering and fft routines
		resultsList = list(DateTime = temp$DateTime[1],
				Sensor = sensor,
				BPMfft = NA,
				BPMforecast = NA,
				BPMpeaks = NA,
				BPMagreeQuality.fft.forecast = NA,
				BPMagreeQuality.prac.forecast = NA,
				BPMagreeQuality.prac.fft = NA,
				BPMforecastflag = 'FAIL',
				WeakSignalFlag = 'FAIL',
				FilteredAmplitude = NA,
				finalBPM = NA,
				finalfilter = NA,
				QA = FALSE
				)
	}
	

	return(resultsList)
}

```


```{r importTrial1heart, echo=showcode}

# TODO: 
# * Import a concatenated heart file from one of the 'good' mussels that also have
# usable gape data
# * Truncate heart data to the time period of the trial (to speed processing)
# * Run through the automated heart rate algorithm
# * Quality check the heart rate estimates and potentially visually inspect
# * Generate time-stamped set of heart rate estimates for each mussel

# control mussels SN173, SN196, and SN224. 
# low salinity mussels SN189, SN218, and SN179

# Reading in a 200MB+ text file takes longer than you'd think
SN173heart <- read.csv(file=paste0(localpath,outputheartdir,'SN173_heart.csv'), 
		na.strings = 'NA') 
# Convert to POSIXct time stamps. Use the %OS option to retain milliseconds values
SN173heart$DateTime <- as.POSIXct(SN173heart$DateTime,
		format = '%Y-%m-%d %H:%M:%OS', tz='UTC')

```

```{r processHeartData, echo=showcode, eval = FALSE}


# Figure out where the start of each 30-second sampling bout should be. 
# The sampling bouts were typically separated by 5 minutes, so we'll just
# look for any time gap that's bigger than 60 seconds. 
steps = diff(SN173heart$DateTime)
bigsteps = which(steps > 60)
bigsteps = bigsteps+1
bigsteps = c(1,bigsteps) # A set of row indices in Heart for each new sample period


# mylowband- filters low freq signals out, gets rid of slow signals
mylowband = 1/100  # filtering - ran some at 1/100 as a starting point.
# 1/100 filters out more low freq stuff.Can go as low as 1/20, try 1/50 next
#myhighband-get rid of high freq noise so you just get normal peaks. 
# lower (1/20-->1/15) low band if have low long dips but actually have 
# quick peaks in there. Bc the smaller it gets, then only lets 
# through small range of freq
myhighband = 1/15  # filtering - changing this has the largest effect, start around 1/15
#1/5 letting more high freq peaks through. may also catch every little peak.
myThreshold = 4 # Minimum agreement between BPM estimates

pb = txtProgressBar(min = 0, max = length(bigsteps), style = 3)

for (i in 1:length(bigsteps)){
	setTxtProgressBar(pb,i)
	res1 = getBPM(SN173heart, t1 = bigsteps[i], sensor = 'IR', chunklength = 400,
			lowband = mylowband, highband = myhighband, amplitudeThreshold = 20,
			BPMqualityThreshold = myThreshold,
			maxBPM = 80, Fs = 0.125, plot = FALSE)
	if (i == 1) {			
		SensorResult = as.data.frame(res1)
		SensorResult[2:length(bigsteps),] = NA
	} else {
		SensorResult[i,] = as.data.frame(res1)
	}
}
close(pb)


```


```{r processHeartDataLoop, echo=showcode, eval=FALSE}
# one-time run of all heart files, will take a few hours


# Define the list of serial numbers to process
mySNs = c('SN173','SN196','SN224','SN189','SN218','SN179','SN214','SN242',
		'SN243','SN198')


for (sn in 1:length(mySNs)){
	
	# Load the concatenated heart file
	SensorData = read.csv(file= paste0(localpath,outputheartdir,
					mySNs[sn],'_heart.csv'), na.strings = 'NA')
	SensorData$DateTime <- as.POSIXct(SensorData$DateTime,
			format = '%Y-%m-%d %H:%M:%OS', tz='UTC')
	
	# Figure out where the start of each 30-second sampling bout should be. 
	# The sampling bouts were typically separated by 5 minutes, so we'll just
	# look for any time gap that's bigger than 60 seconds. 
	steps = diff(SensorData$DateTime)
	bigsteps = which(steps > 60)
	bigsteps = bigsteps+1
	bigsteps = c(1,bigsteps) # A set of row indices in SensorData for each new 
	# sample period
	
	
	# mylowband- filters low freq signals out, gets rid of slow signals
	mylowband = 1/100  # filtering - ran some at 1/100 as a starting point.
	# 1/100 filters out more low freq stuff.Can go as low as 1/20, try 1/50 next
	#myhighband-get rid of high freq noise so you just get normal peaks. 
	# lower (1/20-->1/15) low band if have low long dips but actually have 
	# quick peaks in there. Bc the smaller it gets, then only lets 
	# through small range of freq
	myhighband = 1/15  # filtering - changing this has the largest effect, start around 1/15
	#1/5 letting more high freq peaks through. may also catch every little peak.
	myThreshold = 4 # Minimum agreement between BPM estimates
	
	pb = txtProgressBar(min = 0, max = length(bigsteps), style = 3)
	
	for (i in 1:length(bigsteps)){
		setTxtProgressBar(pb,i)
		res1 = getBPM(SensorData, t1 = bigsteps[i], sensor = 'IR', chunklength = 400,
				lowband = mylowband, highband = myhighband, amplitudeThreshold = 20,
				BPMqualityThreshold = myThreshold,
				maxBPM = 50, Fs = 0.125, plot = FALSE)
		if (i == 1) {			
			SensorResult = as.data.frame(res1)
			SensorResult[2:length(bigsteps),] = NA
		} else {
			SensorResult[i,] = as.data.frame(res1)
		}
	}
	close(pb)

	rm(res1, SensorData)
	
	# Save an output file for faster loading in next chunks
	write.csv(SensorResult,
			file = paste0(localpath,outputheartratedir,mySNs[sn],'_bpm.csv'),  
			row.names=FALSE)

} # end of sensor loop

```


